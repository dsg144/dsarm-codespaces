{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8893776e",
   "metadata": {},
   "source": [
    "# Case 1: Sparse Genetic Architecture (Few Strong-Effect SNPs)\n",
    "\n",
    "We will simulate a simple genetic trait where only a few (5) out of 1000 SNPs really matter. Those few SNPs have noticeable (moderate) effects. Because only a handful matter, they are easier to spot and a score built from just them can predict the trait well.\n",
    "\n",
    "What you will learn:\n",
    "- How we create (simulate) genotype data (numbers 0,1,2 for each SNP)\n",
    "- How we build a trait (phenotype) from a few “causal” SNPs plus random noise\n",
    "- Why we standardize (turn values into z-scores)\n",
    "- How a simple genome-wide scan finds important SNPs\n",
    "- How to build a Polygenic Score (PGS)\n",
    "- How to see if the PGS is useful (decile plot & variance explained)\n",
    "\n",
    "Key vocabulary (plain words):\n",
    "- SNP: A spot in the genome that can vary between people\n",
    "- Genotype value: 0, 1, or 2 copies of the effect allele\n",
    "- Phenotype: The trait we measure (simulated)\n",
    "- Causal SNP: A SNP we chose to actually influence the trait\n",
    "- Noise: Random variation not explained by genetics\n",
    "\n",
    "Try asking Copilot (beginner prompts):\n",
    "- \"What does 0/1/2 mean in genotype data?\"\n",
    "- \"Why do we add noise to the phenotype?\"\n",
    "- \"Explain what a causal SNP is in simple terms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f852ca1",
   "metadata": {},
   "source": [
    "### Step 0: Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Imports & Base Parameters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# For nicer plots in some environments\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f582a5",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Genotypes & Phenotypes\n",
    "\n",
    "**Configuration (dataclass):**  \n",
    "We define all simulation knobs once:\n",
    "\n",
    "- num_snps: total SNPs (default 1000)  \n",
    "- num_individuals: people in discovery & target (each)  \n",
    "- num_causal: how many SNPs truly affect the phenotype (first indices)  \n",
    "- effect_size: per‑allele additive effect for each causal SNP  \n",
    "- noise_std: standard deviation of random noise added to phenotype  \n",
    "- n_permutations: how many shuffles to build the empirical threshold to determine which SNPs to include in the PGS \n",
    "\n",
    "(You can change these in the SimulationConfig block below and re‑run.)\n",
    "\n",
    "**What are we making?**  \n",
    "We’re creating a simple, realistic genetic dataset for learning. Each “person” gets:\n",
    "- **Genotypes:** 0 / 1 / 2 copies of an effect allele at 1,000 SNPs\n",
    "- **Phenotype:** Built from a few causal SNPs + noise\n",
    "\n",
    "**Why two groups?**\n",
    "We split the data so we can discover patterns on one set and test them fairly on a fresh set.\n",
    "\n",
    "- **Discovery set:** find SNP–trait signals to create the PGS score \n",
    "- **Target set:** used for honest evaluation of the PGS score\n",
    "\n",
    "**How do we simulate?**\n",
    "1. Pick allele frequency per SNP  \n",
    "2. Draw genotypes (0/1/2) via binomial draws  \n",
    "3. Mark first `num_causal` SNPs as causal to represent our phenotype \n",
    "4. Phenotype = sum(effect_size * genotype at causal SNPs) + noise  \n",
    "\n",
    "---\n",
    "\n",
    "**Beginner prompts (try these for deeper understanding):**\n",
    "- \"How does allele frequency affect the chance of 0/1/2?\"\n",
    "- \"What happens if I change the number of causal SNPs?\"\n",
    "- \"Why do we need separate discovery and target sets?\"\n",
    "- \"What real-world traits might follow this 'few causal SNPs' pattern?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6dbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Config + Simulation + Train Preview (single cell)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Configuration ---\n",
    "@dataclass\n",
    "class SimulationConfig:\n",
    "    num_snps: int = 1000\n",
    "    num_individuals: int = 1000\n",
    "    num_causal: int = 5\n",
    "    effect_size: float = 0.5   # per allele effect for each causal SNP\n",
    "    noise_std: float = 1.0\n",
    "    n_permutations: int = 100  # used later for permutation threshold\n",
    "    broad_PGS_k: int = 100     # used later for optional broad PGS\n",
    "\n",
    "config = SimulationConfig()\n",
    "\n",
    "# --- Simulate Genotype & Phenotype (Discovery/Target) ---\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# Causal SNP indices (first num_causal)\n",
    "causal_indices = np.arange(config.num_causal)\n",
    "\n",
    "# Allele frequencies (0.1 to 0.5)\n",
    "allele_freqs = rng.uniform(0.1, 0.5, size=config.num_snps)\n",
    "\n",
    "# Genotypes 0/1/2 for train and test\n",
    "geno_train = np.empty((config.num_individuals, config.num_snps), dtype=np.int8)\n",
    "geno_test  = np.empty_like(geno_train)\n",
    "for j, p in enumerate(allele_freqs):\n",
    "    geno_train[:, j] = rng.binomial(2, p, size=config.num_individuals)\n",
    "    geno_test[:, j]  = rng.binomial(2, p, size=config.num_individuals)\n",
    "\n",
    "# Phenotypes: sum over causal SNPs + noise\n",
    "phen_train = np.zeros(config.num_individuals, dtype=float)\n",
    "phen_test  = np.zeros(config.num_individuals, dtype=float)\n",
    "for snp in causal_indices:\n",
    "    phen_train += config.effect_size * geno_train[:, snp]\n",
    "    phen_test  += config.effect_size * geno_test[:, snp]\n",
    "phen_train += rng.normal(0, config.noise_std, size=config.num_individuals)\n",
    "phen_test  += rng.normal(0, config.noise_std, size=config.num_individuals)\n",
    "\n",
    "# Package for later steps\n",
    "data = {\n",
    "    \"allele_freqs\": allele_freqs,\n",
    "    \"geno_train\": geno_train,\n",
    "    \"geno_test\": geno_test,\n",
    "    \"phen_train\": phen_train,\n",
    "    \"phen_test\": phen_test,\n",
    "    \"causal_snps\": causal_indices,\n",
    "}\n",
    "\n",
    "print(f\"Train phenotype mean/var: {phen_train.mean():.3f} / {phen_train.var():.3f}\")\n",
    "print(f\"Causal SNP indices: {data['causal_snps']}\")\n",
    "\n",
    "# --- Preview (TRAIN ONLY) ---\n",
    "n_rows = 5\n",
    "n_snps = 8\n",
    "snp_cols = [f\"SNP_{i:04d}\" for i in range(n_snps)]\n",
    "\n",
    "df_train = pd.DataFrame(data[\"geno_train\"][:n_rows, :n_snps], columns=snp_cols)\n",
    "df_train[\"phen_train\"] = data[\"phen_train\"][:n_rows]\n",
    "\n",
    "print(\"\\nTrain (wide) first rows:\")\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb5dcf",
   "metadata": {},
   "source": [
    "### Interpreting the Training Set Preview\n",
    "\n",
    "You’re seeing only the discovery (training) table. The target (test) data exists but isn’t shown here.\n",
    "\n",
    "- Columns\n",
    "  - SNP_0000, SNP_0001, …: Genotypes coded 0/1/2 (0=none, 1=heterozygous, 2=homozygous effect).\n",
    "  - phen_train: Simulated trait = sum(effect_size × genotype at causal SNPs) + random noise. Higher = “more” of the trait.\n",
    "\n",
    "- Reading a row (example)\n",
    "  - SNP_0000=1, SNP_0001=2, …, phen_train=3.47 → genotypes at causal SNPs plus noise produce a phenotype of 3.47.\n",
    "\n",
    "- Key points\n",
    "  - Only the first num_causal SNPs truly affect the phenotype; others are noise.\n",
    "  - Train and test are separate draws; don’t use test data until evaluation.\n",
    "\n",
    "- effect_size (what it means)\n",
    "  - Per-allele additive change in the phenotype (slope). Moving 0→1→2 copies adds ≈ effect_size each step.\n",
    "  - Simulation vs real data: in this notebook we choose effect_size (β). In real studies, β is unknown and estimated (β̂) via GWAS (per‑SNP regression), with standard errors and p‑values.\n",
    "  - Larger |effect_size| generally produces bigger |r| and higher PRS R², but detectability also depends on sample size, allele frequency, and noise. In this notebook we use r as a simple proxy weight for β̂.\n",
    "\n",
    "- Quick checks\n",
    "  - Genotypes are only 0/1/2.\n",
    "  - Train and test (when viewed) should be on similar scales given the same settings.\n",
    "  - Causal SNPs may not be visually obvious; association appears via correlation.\n",
    "\n",
    "- Next\n",
    "  - Standardize (z-score) to compare SNP–trait correlations on a common scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cfc22",
   "metadata": {},
   "source": [
    "### Step 2: Standardization\n",
    "\n",
    "We now convert raw values into **z-scores** (also called standardization) in the **discovery set** (training data).\n",
    "\n",
    "**What is a z-score (plain words)?**  \n",
    "“How many standard deviations above (+) or below (–) the mean is this value?”\n",
    "\n",
    "**Formula:**  \n",
    "z = (value − mean) / standard_deviation\n",
    "\n",
    "**Why we do this before correlations:**\n",
    "- Puts every SNP column and the phenotype on the *same scale* (mean 0, SD 1)\n",
    "- Makes Pearson r just the **average product** of two z-scored variables (acts like an effect size)\n",
    "- Lets us compare SNP signals fairly (a rare vs common SNP aren’t unfairly scaled)\n",
    "- Keeps the *shape* and *ordering* of values (only shifts & rescales)\n",
    "\n",
    "**What changes vs what stays the same:**\n",
    "- Changes: mean becomes 0, spread becomes 1\n",
    "- Stays the same: who is larger/smaller than whom; histogram shape (aside from axis numbers)\n",
    "\n",
    "**Tiny example:**  \n",
    "Raw values: 2, 4, 6 (mean=4, SD≈1.633).  \n",
    "z for 6 = (6 − 4)/1.633 ≈ +1.23 → “1.23 SD above average.”\n",
    "\n",
    "**Intuition:**  \n",
    "Standardization is like switching from different local units (inches, meters, feet) to a common “distance-from-average” unit so comparisons are clean.\n",
    "\n",
    "**Optional prompts to explore:**\n",
    "- “Why doesn’t standardization change ordering?”\n",
    "- “Why is correlation simpler after z-scoring both variables?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize Genotypes and Phenotypes\n",
    "\n",
    "def standardize_matrix(mat: np.ndarray):  # -> Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "    mean = mat.mean(axis=0)\n",
    "    std = mat.std(axis=0)\n",
    "    std[std == 0] = 1.0\n",
    "    return (mat - mean) / std, mean, std\n",
    "\n",
    "\n",
    "Z_geno_train, geno_mean_train, geno_std_train = standardize_matrix(data['geno_train'])\n",
    "Z_geno_test, geno_mean_test_raw, geno_std_test_raw = standardize_matrix(data['geno_test'])  # independent standardization\n",
    "\n",
    "phen_train = data['phen_train']\n",
    "phen_test = data['phen_test']\n",
    "Z_phen_train = (phen_train - phen_train.mean()) / phen_train.std()\n",
    "Z_phen_test = (phen_test - phen_test.mean()) / phen_test.std()\n",
    "\n",
    "# Diagnostics to show why raw vs standardized look similar\n",
    "print(\"Raw phenotype   mean/std = {:.3f} / {:.3f}\".format(phen_train.mean(), phen_train.std()))\n",
    "print(\"Standardized    mean/std = {:.3f} / {:.3f}\".format(Z_phen_train.mean(), Z_phen_train.std()))\n",
    "print(\"Raw   min/max = {:.3f} / {:.3f}\".format(phen_train.min(), phen_train.max()))\n",
    "print(\"Z     min/max = {:.3f} / {:.3f}\".format(Z_phen_train.min(), Z_phen_train.max()))\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "# Raw distribution\n",
    "axes[0].hist(phen_train, bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(phen_train.mean(), color='k', linestyle='--', linewidth=1, label='Mean')\n",
    "axes[0].set_title('Raw Phenotype (Discovery)')\n",
    "axes[0].set_xlabel('Phenotype')\n",
    "axes[0].legend()\n",
    "\n",
    "# Standardized distribution (fixed axis to emphasize z-scale)\n",
    "axes[1].hist(Z_phen_train, bins=30, color='salmon', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].set_xlim(-4,4)\n",
    "axes[1].set_title('Standardized Phenotype (Discovery)')\n",
    "axes[1].set_xlabel('Z-Phenotype')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd830971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional viz: one SNP before/after standardization (discovery set)\n",
    "# Pick a SNP with allele frequency near 0.30 for variability\n",
    "p = data['allele_freqs']\n",
    "idx_snp = int(np.argmin(np.abs(p - 0.30)))\n",
    "x_raw = data['geno_train'][:, idx_snp]\n",
    "x_z = Z_geno_train[:, idx_snp]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "# Left: raw genotype distribution (0/1/2)\n",
    "axes[0].hist(x_raw, bins=[-0.5, 0.5, 1.5, 2.5], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xticks([0,1,2])\n",
    "axes[0].axvline(x_raw.mean(), color='k', linestyle='--', linewidth=1, label=f\"Mean={x_raw.mean():.2f}\")\n",
    "axes[0].set_title(f\"SNP_{idx_snp:04d} (raw)\")\n",
    "axes[0].set_xlabel('Genotype (0/1/2)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(frameon=False)\n",
    "\n",
    "# Right: standardized genotype distribution (z-scores)\n",
    "axes[1].hist(x_z, bins=30, color='indianred', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].axvline(1, color='gray', linestyle=':', linewidth=1, label='±1 SD')\n",
    "axes[1].axvline(-1, color='gray', linestyle=':', linewidth=1)\n",
    "axes[1].set_title(f\"SNP_{idx_snp:04d} (standardized)\")\n",
    "axes[1].set_xlabel('Z-score')\n",
    "axes[1].legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59acb582",
   "metadata": {},
   "source": [
    "### Step 3: GWAS Correlations\n",
    "\n",
    "**Goal.** See which SNPs move with (are associated with) the trait (phenotype).\n",
    "\n",
    "**How we compute.** With z-scores, the per-SNP Pearson correlation is the average product:\n",
    "$$\n",
    "r_j = \\frac{1}{N}\\sum_{i=1}^{N} G^{(z)}_{ij}\\,y^{(z)}_i\n",
    "$$\n",
    "Where N = number of individuals, G^{(z)}_{ij} = standardized genotype at SNP j, y^{(z)}_i = standardized phenotype.\n",
    "\n",
    "**Reading r.**\n",
    "- r_j ≈ 0: no detectable association\n",
    "- r_j > 0: higher genotype → higher trait\n",
    "- r_j < 0: higher genotype → lower trait\n",
    "- Larger |r_j| ⇒ stronger SNP–trait link (r_j² is in-sample variance explained by SNP j)\n",
    "\n",
    "**What to expect in Case 1 (sparse).** Because only 5 SNPs matter, those should show noticeably larger |r| than the rest.\n",
    "\n",
    "**Next.** Use permutations to see how large the max |r| can be by chance and set a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f452b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: GWAS via Per-SNP Correlations\n",
    "r_values = (Z_geno_train * Z_phen_train[:, None]).mean(axis=0)\n",
    "r_abs = np.abs(r_values)\n",
    "max_r = r_abs.max()\n",
    "max_r_snp = r_abs.argmax()\n",
    "count_modest = (r_abs > 0.1).sum()\n",
    "print(f\"Maximum |r| = {max_r:.3f} at SNP {max_r_snp}\")\n",
    "print(f\"Number of SNPs with |r| > 0.1: {count_modest} / {config.num_snps}\")\n",
    "print(f\"Causal SNPs: {data['causal_snps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f116973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Step 3 viz: one SNP vs phenotype (discovery set)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = int(np.argmax(r_abs))  # SNP with largest |r|\n",
    "snp_name = f\"SNP_{idx:04d}\"\n",
    "\n",
    "x_raw = data['geno_train'][:, idx]\n",
    "y_raw = phen_train\n",
    "x_z = Z_geno_train[:, idx]\n",
    "y_z = Z_phen_train\n",
    "r = float(r_values[idx]); r2 = r*r\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "jit = (rng.random(x_raw.size) - 0.5) * 0.12  # jitter to separate 0/1/2 columns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Raw scale: shows per-allele shifts\n",
    "axes[0].scatter(x_raw + jit, y_raw, s=8, alpha=0.5)\n",
    "means = [y_raw[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[0].plot([0,1,2], means, 'r-o', lw=2, label='Group means')\n",
    "axes[0].set_xticks([0,1,2]); axes[0].legend(frameon=False)\n",
    "axes[0].set_xlabel('Genotype (0/1/2)'); axes[0].set_ylabel('Phenotype (raw)')\n",
    "axes[0].set_title(f'{snp_name} vs phen_train (raw)')\n",
    "\n",
    "# Z scale: matches how r is computed\n",
    "axes[1].scatter(x_z + jit, y_z, s=8, alpha=0.5)\n",
    "# Group means in z space (plot at mean x_z for each genotype 0/1/2)\n",
    "x_means = [x_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "y_means = [y_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[1].plot(x_means, y_means, 'r-o', lw=2, label='Group means')\n",
    "axes[1].legend(frameon=False)\n",
    "axes[1].set_xlabel('Genotype (z-scored)'); axes[1].set_ylabel('Phenotype (z-scored)')\n",
    "axes[1].set_title(f'{snp_name} (|r|={abs(r):.2f}, R²={r2:.2f})')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328dc3ca",
   "metadata": {},
   "source": [
    "### How the scatterplot connects to Step 3 correlations\n",
    "\n",
    "- Left panel (raw):\n",
    "  - Three vertical bands (genotype 0/1/2). The red “group means” line shows the average phenotype for each genotype.\n",
    "  - A steeper upward line means a larger per‑allele shift in the raw phenotype (mirroring the causal effect_size β in our simulation).\n",
    "\n",
    "- Right panel (z-scored):\n",
    "  - Both axes are standardized, which is exactly how we compute r in Step 3.\n",
    "  - Pearson r for this SNP is the average product r = mean(x_z · y_z). A tighter, more tilted cloud (and a rising red line) yields larger |r|.\n",
    "  - The title displays this SNP’s r and R² = r² (in‑sample variance explained by this SNP).\n",
    "\n",
    "- Why this matters:\n",
    "  - The r shown here is the same r_values[idx] used in the Manhattan plot and later as the PRS weight for this SNP.\n",
    "  - Larger |r| places a higher dot in the Manhattan plot, increases the chance of selection, and gives more weight in the PRS.\n",
    "\n",
    "Note: Jitter only separates overlapping 0/1/2 points visually; it does not change r."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32f74",
   "metadata": {},
   "source": [
    "### Mini-GWAS framing (what we’re mimicking)\n",
    "\n",
    "- A GWAS tests each SNP across the genome for association with a phenotype (one SNP at a time).  \n",
    "- Real GWAS uses regression (**β, SE, p-value**) and includes covariates (age, sex, ancestry PCs) to control confounding.  \n",
    "- In this exercise, we use **z-scores** and compute a simple **per-SNP correlation (r)** as a stand-in for GWAS effect size.  \n",
    "- Output of this step is a “summary stats–like” table: **SNP ID, r, |r|**.  \n",
    "- Next, we’ll visualize all SNPs together (Manhattan plot) and set a significance cut line via **permutation**—analogous to GWAS genome-wide thresholds.  \n",
    "\n",
    "**TL;DR:** Step 3 ≈ a **mini-GWAS** pass over SNPs to get per-SNP effects we can carry forward.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f69a3c",
   "metadata": {},
   "source": [
    "### Step 4: Permutation (simple) + Manhattan Plot\n",
    "\n",
    "From one SNP to all SNPs\n",
    "- The Step 3 scatterplot shows how a single SNP’s correlation (r) is computed on z-scored data.\n",
    "- The Manhattan plot stacks this same |r| for every SNP to give a genome-wide view.\n",
    "\n",
    "What we do (simple permutation)\n",
    "- We ask: “How tall could the biggest |r| be just by chance?”\n",
    "- Shuffle the phenotype B times.\n",
    "- Each time, compute |r| across all SNPs and record the maximum.\n",
    "- Use the 95th percentile of these maxima as the threshold line (controls the “tallest-by-chance” effect when scanning many SNPs).\n",
    "\n",
    "How to read the Manhattan plot\n",
    "- Dot height = |r| for a SNP; the dashed orange line is the permutation-based threshold.\n",
    "- Orange dots = SNPs above the threshold (selected candidates).\n",
    "- Red stars = true causal SNPs (unknown in real data).\n",
    "- In this sparse case, a few clear peaks should rise above the background.\n",
    "\n",
    "Beginner tip\n",
    "- Like a “tallest building contest” among 1000 buildings: the threshold sits just above how tall the tallest would be under random heights. Peaks well above it are likely real.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compact permutation + Manhattan (single threshold line)\n",
    "def perm_threshold(Z_geno: np.ndarray, y: np.ndarray, B: int = 100, q: float = 0.95, seed: int = 0) -> float:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_abs = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        y_shuf = rng.permutation(y)\n",
    "        r_shuf = (Z_geno * y_shuf[:, None]).mean(axis=0)\n",
    "        max_abs[b] = np.abs(r_shuf).max()\n",
    "    return float(np.quantile(max_abs, q))\n",
    "\n",
    "ALPHA = 0.05\n",
    "B = config.n_permutations if hasattr(config, \"n_permutations\") else 100\n",
    "threshold = perm_threshold(Z_geno_train, Z_phen_train, B=B, q=1-ALPHA, seed=0)\n",
    "selected_idx = np.flatnonzero(r_abs >= threshold)\n",
    "\n",
    "print(f\"Permutation threshold (95%): {threshold:.3f} | Selected SNPs: {selected_idx.size}\")\n",
    "\n",
    "# Manhattan plot (simple)\n",
    "plt.figure(figsize=(12, 5))\n",
    "x = np.arange(config.num_snps)\n",
    "plt.scatter(x, r_abs, c='black', s=8, alpha=0.7, label='All SNPs')\n",
    "if selected_idx.size:\n",
    "    plt.scatter(selected_idx, r_abs[selected_idx], c='tab:orange', s=18, alpha=0.9, label='Selected (≥ threshold)')\n",
    "plt.scatter(data['causal_snps'], r_abs[data['causal_snps']], c='red', s=80, marker='*', label='True causal SNPs')\n",
    "plt.axhline(threshold, color='orange', linestyle='--', linewidth=1.5, label=f'95% perm threshold = {threshold:.3f}')\n",
    "plt.xlabel('SNP Index'); plt.ylabel('|r| (association strength)')\n",
    "plt.title('Manhattan Plot: |r| per SNP with Empirical Threshold')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdafc7",
   "metadata": {},
   "source": [
    "### From GWAS results to Manhattan (our version)\n",
    "\n",
    "- Classic GWAS Manhattan plots **−log10(p)** by genomic position; taller peaks = stronger evidence.  \n",
    "- Here, we plot **|r|** instead of −log10(p). The goal is the same: a genome-wide view of signal strength.  \n",
    "- GWAS uses fixed significance lines (e.g., *5×10⁻⁸*). We use a **permutation-based line** that reflects our data.  \n",
    "- Selection rule: take all SNPs above the line; if none cross, use a **top-K by |r| fallback** (keep the sign of r for direction).  \n",
    "- Same idea, simpler ingredients: our permutation line plays the role of a **GWAS significance threshold**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3d6be",
   "metadata": {},
   "source": [
    "### Step 5: Build and Evaluate a Polygenic Score\n",
    "\n",
    "**Goal:** Turn SNP-trait associations into a predictive score and evaluate performance.\n",
    "\n",
    "**What we do:**\n",
    "1. **Select SNPs:** Use SNPs that passed the permutation threshold from Step 4\n",
    "2. **Fallback strategy:** If no SNPs pass threshold, use top-K SNPs by |r| (K=100)\n",
    "3. **Calculate PRS:** Weighted sum of standardized genotypes (weights = correlations from discovery)\n",
    "4. **Evaluate performance:** Correlation metrics (r, R²) and decile stratification\n",
    "5. **Visualize:** Scatter plot and decile plot showing PRS-phenotype relationship\n",
    "\n",
    "The decile plot is particularly useful for seeing the practical impact of the PRS - it shows how the average trait value differs across individuals grouped by their genetic risk score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build and Evaluate Polygenic Score\n",
    "\n",
    "# Primary selection: permutation-thresholded set\n",
    "selected_idx = np.flatnonzero(r_abs >= threshold)\n",
    "label = \"Selected (perm-threshold)\"\n",
    "\n",
    "# Fallback to top-K by |r| if nothing selected\n",
    "if selected_idx.size == 0:\n",
    "    selected_idx = np.argsort(r_abs)[-topK:]\n",
    "    label = f\"Top-{topK} by |r| (fallback)\"\n",
    "\n",
    "# Build PRS with training weights (r-values) on standardized target genotypes\n",
    "prs_raw = Z_geno_test[:, selected_idx] @ r_values[selected_idx]\n",
    "\n",
    "# Standardize PRS\n",
    "prs = (prs_raw - prs_raw.mean()) / prs_raw.std()\n",
    "\n",
    "# Evaluate against standardized target phenotype\n",
    "R = float(np.corrcoef(prs, Z_phen_test)[0, 1])\n",
    "R2 = R * R\n",
    "\n",
    "print(f\"PRS type: {label}\")\n",
    "print(f\"SNPs used: {selected_idx.size} | Pearson r = {R:.3f} | R^2 = {R2:.3f}\")\n",
    "\n",
    "# Visualization: Scatter plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(prs, Z_phen_test, s=8, alpha=0.5)\n",
    "plt.xlabel('PRS (z-score)')\n",
    "plt.ylabel('Phenotype (z-score)')\n",
    "plt.title(f'PRS vs Phenotype (r={R:.2f}, R²={R2:.2f})')\n",
    "\n",
    "# Visualization: Decile plot\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create deciles\n",
    "edges = np.quantile(prs, np.linspace(0, 1, 11))\n",
    "dec = np.digitize(prs, edges[1:-1], right=True)\n",
    "mean_per_decile = [Z_phen_test[dec == d].mean() if np.sum(dec==d) > 0 else np.nan for d in range(10)]\n",
    "se_per_decile = [Z_phen_test[dec == d].std(ddof=1)/np.sqrt(np.sum(dec==d)) \n",
    "                if np.sum(dec==d) > 1 else np.nan for d in range(10)]\n",
    "gap = mean_per_decile[-1] - mean_per_decile[0] if not np.isnan(mean_per_decile[0]) and not np.isnan(mean_per_decile[-1]) else np.nan\n",
    "\n",
    "plt.errorbar(range(1, 11), mean_per_decile, yerr=se_per_decile, fmt='-o', capsize=3)\n",
    "plt.title(f'Phenotype by PRS Decile (Δ₁₀–₁ ≈ {gap:.2f} SD)')\n",
    "plt.xlabel('PRS Decile (1=lowest, 10=highest)')\n",
    "plt.ylabel('Mean Phenotype (z-score)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab50e2",
   "metadata": {},
   "source": [
    "### Step 5 Interpretation\n",
    "\n",
    "**Performance metrics:**\n",
    "- PRS type: Selected (perm-threshold)\n",
    "- SNPs used: 5\n",
    "- Correlation: r = 0.557, R² = 0.310\n",
    "\n",
    "**What this means:**\n",
    "- **Selected SNPs:** Only variants that cleared the permutation threshold are included, reducing false positives\n",
    "- **Strong predictive power:** Explaining ~31% of trait variance with just 5 SNPs is impressive and confirms our sparse architecture\n",
    "- **Direct genetic insight:** The significant SNPs likely represent true causal variants (in real-world data, they might tag causal variants)\n",
    "\n",
    "**Understanding the decile plot:**\n",
    "- Each point shows the average trait value for individuals in that PRS decile\n",
    "- Error bars represent standard error of the mean\n",
    "- The steep slope demonstrates the strong predictive power of the PRS\n",
    "- The gap between lowest and highest deciles (Δ₁₀-₁) shows the practical effect size\n",
    "- In this sparse case, we see a strong, clear gradient across deciles - individuals with high PRS have substantially higher trait values\n",
    "\n",
    "**Why this matters:**\n",
    "The strong performance with few SNPs is characteristic of traits with a simple genetic architecture. This approach efficiently identifies the key genetic drivers and builds a highly predictive, interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a47a8",
   "metadata": {},
   "source": [
    "**Comparison to Other Cases**\n",
    "- Unlike Case 2 (polygenic), signals are strong enough to pass significance threshold\n",
    "- Unlike Case 4 (hybrid), all genetic signal comes from a few large-effect variants\n",
    "- A strict PRS approach works best since additional SNPs add only noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639af3b8",
   "metadata": {},
   "source": [
    "### Conclusion: What We Learned from Case 1\n",
    "\n",
    "**Journey summary**  \n",
    "In this case study, we explored a sparse genetic architecture where only a few variants strongly influence a trait. This mirrors certain real-world traits governed by a small number of impactful genetic factors.\n",
    "\n",
    "**What we accomplished**\n",
    "1. **Simulation setup**: Created genetic data with just 5 causal SNPs (out of 1000) affecting the trait\n",
    "2. **Data preparation**: Standardized all variables to enable fair comparisons on a common scale\n",
    "3. **Signal detection**: Used correlation analysis to identify the SNPs that truly matter\n",
    "4. **Statistical rigor**: Established an empirical significance threshold through permutation testing\n",
    "5. **Visual insight**: Demonstrated how causal SNPs stand out as peaks in a Manhattan plot\n",
    "6. **Prediction model**: Built a focused polygenic score using only significant SNPs\n",
    "7. **Validation**: Confirmed strong predictive power (~31% variance explained) in independent data\n",
    "\n",
    "**Key insights**\n",
    "- In sparse architectures, true signals can be clearly distinguished from background noise\n",
    "- Permutation-based thresholds provide protection against false positives\n",
    "- A small set of correctly identified SNPs can deliver substantial predictive power\n",
    "- Standardization creates a level playing field for comparing and weighting genetic effects\n",
    "\n",
    "**Why this matters**\n",
    "This approach excels for traits with simple genetic architectures—like certain Mendelian disorders or traits influenced by a few major loci. The ability to pinpoint specific causal variants makes these models both interpretable and potentially actionable in clinical settings.\n",
    "\n",
    "**Taking it further**\n",
    "What happens when a trait is influenced by hundreds of variants each with tiny effects? This polygenic scenario requires different strategies, which we explore in Case 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d404ae1",
   "metadata": {},
   "source": [
    "# Complete the Reflection & Comparison Questions in Shared Slides in Groups in Canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
