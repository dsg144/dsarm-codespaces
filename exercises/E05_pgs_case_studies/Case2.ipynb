{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d03bf6",
   "metadata": {},
   "source": [
    "# Case 2: Diffuse Polygenicity (Many Tiny Effects)\n",
    "\n",
    "This notebook follows the GWAS→PRS workflow but simulates a trait with many tiny genetic effects.  Unlike Case 1, no single SNP will stand out as genome‑wide significant, yet aggregating weak signals into a polygenic score yields modest predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e8b8b",
   "metadata": {},
   "source": [
    "### Step 0: Imports\n",
    "\n",
    "Import required libraries for simulation, computation and plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730903ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display\n",
    "\n",
    "# Style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46069ec9",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Genotypes & Phenotypes\n",
    "\n",
    "This case simulates a highly polygenic trait: many SNPs each contribute a tiny effect. We split individuals 70/30 into discovery (training) and target (testing) sets, and scale environmental noise so that the trait’s narrow‑sense heritability is about 30%.\n",
    "\n",
    "**Configuration (dataclass):**\n",
    "We define all simulation knobs once:\n",
    "\n",
    "- num_snps: total SNPs (default 1000)\n",
    "- num_individuals: total people (split 70/30 into discovery/target)\n",
    "- num_causal: how many SNPs truly affect the phenotype (randomly chosen indices)\n",
    "- effect_size: SD for tiny additive effects of causal SNPs (effects drawn ~ N(0, effect_size²) with mild clipping and re-scaling)\n",
    "- n_permutations: number of shuffles to build the empirical max |r| threshold (used in Step 4)\n",
    "- broad_PGS_k: how many top-|r| SNPs to aggregate into the PRS (used in Step 5)\n",
    "- noise_std: present in the config; noise is actually scaled to hit a target heritability (H2_TARGET ≈ 0.30)\n",
    "\n",
    "(You can change these in the SimulationConfig block below and re‑run.)\n",
    "\n",
    "**What are we making?**\n",
    "A simple, realistic dataset for learning polygenic prediction. Each “person” gets:\n",
    "- Genotypes: 0 / 1 / 2 copies of an allele at 1,000 SNPs\n",
    "- Phenotype: Genetic component from many tiny-effect causal SNPs + environmental noise scaled to target h² ≈ 0.30\n",
    "\n",
    "**Why two groups?**\n",
    "We discover patterns on one set and test them fairly on a fresh set.\n",
    "- Discovery set (train): estimate SNP–trait associations used to create the PRS\n",
    "- Target set (test): honest evaluation of the PRS\n",
    "\n",
    "**How do we simulate?**\n",
    "1) Draw an allele frequency for each SNP (uniform between 0.05 and 0.5).  \n",
    "2) Generate genotypes (0/1/2) via binomial draws given each SNP’s allele frequency.  \n",
    "3) Randomly choose `num_causal` causal SNPs; draw tiny effects for them ~ N(0, effect_size²), clip to ±2×effect_size, and re-standardize to keep the intended SD.  \n",
    "4) Compute the genetic component g = G·β.  \n",
    "5) Scale environmental noise so that h² ≈ 0.30 in the discovery set, then form phenotype = g + noise.  \n",
    "6) Use the discovery set to compute SNP–trait correlations; the target set is held out for PRS evaluation.\n",
    "\n",
    "Note: In a diffuse polygenic architecture, no single SNP is expected to clear a strict significance threshold; the useful information lies in the ranking of many small signals that we aggregate into a PRS.\n",
    "\n",
    "**Beginner prompts (try these for deeper understanding):**\n",
    "- “How do allele frequencies determine the probabilities of 0/1/2 genotypes?”  \n",
    "- “What happens if I change the number of causal SNPs (e.g., 50 vs 300 vs 600)?”  \n",
    "- “How does the target heritability (H2_TARGET) affect PRS performance?”  \n",
    "- “Why do many tiny effects make it hard for any single SNP to be significant?”  \n",
    "- “Why do we need separate discovery and target sets?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Config + Polygenic Simulation + Train Preview\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "@dataclass\n",
    "class SimulationConfig:\n",
    "    num_snps: int = 1000\n",
    "    num_individuals: int = 600\n",
    "    num_causal: int = 300\n",
    "    effect_size: float = 0.02\n",
    "    noise_std: float = 1.0\n",
    "    n_permutations: int = 100\n",
    "    broad_PGS_k: int = 50\n",
    "\n",
    "config = SimulationConfig()\n",
    "rng = np.random.default_rng(42)\n",
    "H2_TARGET = 0.30\n",
    "\n",
    "# 1. Draw allele frequencies and genotypes (0/1/2)\n",
    "allele_freqs = rng.uniform(0.05, 0.5, size=config.num_snps)\n",
    "geno = np.empty((config.num_individuals, config.num_snps), dtype=np.int8)\n",
    "for j, p in enumerate(allele_freqs):\n",
    "    geno[:, j] = rng.binomial(2, p, size=config.num_individuals)\n",
    "\n",
    "# 2. Split into discovery (train) and target (test) sets\n",
    "n_train = int(0.7 * config.num_individuals)\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, config.num_individuals)\n",
    "geno_train, geno_test = geno[train_idx], geno[test_idx]\n",
    "\n",
    "# 3. Assign causal SNPs and tiny effects\n",
    "causal_idx = rng.choice(config.num_snps, size=config.num_causal, replace=False)\n",
    "beta = np.zeros(config.num_snps)\n",
    "beta_draw = rng.normal(0.0, config.effect_size, size=config.num_causal)\n",
    "beta_draw = np.clip(beta_draw, -2*config.effect_size, 2*config.effect_size)\n",
    "# re-standardize to maintain target SD\n",
    "beta_draw *= config.effect_size / beta_draw.std()\n",
    "beta[causal_idx] = beta_draw\n",
    "\n",
    "# 4. Genetic component and noise scaled to target h^2\n",
    "g_train = geno_train @ beta\n",
    "g_test  = geno_test  @ beta\n",
    "\n",
    "var_g = g_train.var()\n",
    "var_e = max(var_g * (1 - H2_TARGET) / H2_TARGET, 1e-12)\n",
    "env_sd = np.sqrt(var_e)\n",
    "phen_train = g_train + rng.normal(0.0, env_sd, size=n_train)\n",
    "phen_test  = g_test  + rng.normal(0.0, env_sd, size=config.num_individuals - n_train)\n",
    "\n",
    "# Package for later steps\n",
    "data = {\n",
    "    \"allele_freqs\": allele_freqs,\n",
    "    \"geno_train\": geno_train,\n",
    "    \"geno_test\": geno_test,\n",
    "    \"phen_train\": phen_train,\n",
    "    \"phen_test\": phen_test,\n",
    "    \"causal_snps\": np.sort(causal_idx),\n",
    "}\n",
    "\n",
    "# Diagnostics\n",
    "emp_h2 = var_g / (var_g + var_e)\n",
    "print(f\"Train phenotype mean/var: {phen_train.mean():.3f} / {phen_train.var():.3f}\")\n",
    "print(f\"Empirical h^2 ≈ {emp_h2:.3f}\")\n",
    "print(f\"Number of causal SNPs: {len(causal_idx)}\")\n",
    "\n",
    "# Preview first 5 rows\n",
    "preview_snps = 8\n",
    "df_train = pd.DataFrame(geno_train[:5, :preview_snps],\n",
    "                        columns=[f\"SNP_{i:04d}\" for i in range(preview_snps)])\n",
    "df_train[\"phen_train\"] = phen_train[:5]\n",
    "print(\"Train (wide) first rows:\")\n",
    "display(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff7ea6",
   "metadata": {},
   "source": [
    "### Interpreting the Training Set Preview\n",
    "\n",
    "You’re seeing only the discovery (training) table. The target (test) data exists but isn’t shown here.\n",
    "\n",
    "- Columns\n",
    "  - SNP_0000, SNP_0001, …: Genotypes coded 0/1/2 (0=none, 1=heterozygous, 2=homozygous effect).\n",
    "  - phen_train: Simulated trait = genetic component g + noise, where g = Σ_j (β_j × genotype_ij) over many tiny-effect causal SNPs. Noise is scaled so target h² ≈ 0.30. Higher = “more” of the trait.\n",
    "\n",
    "- Reading a row (example)\n",
    "  - SNP_0000=1, SNP_0001=2, …, phen_train=3.47 → genotypes at many causal SNPs (with small β_j of mixed signs) plus noise produce a phenotype of 3.47.\n",
    "\n",
    "- Key points\n",
    "  - Causal SNPs are randomly chosen indices; most β_j are tiny and may be positive or negative.\n",
    "  - Train and test are separate draws; don’t use test data until evaluation.\n",
    "  - In this diffuse polygenic case, no single SNP is expected to be genome‑wide significant; signal is in ranking many small |r|.\n",
    "\n",
    "- effect_size (what it means)\n",
    "  - Per-allele additive change in the phenotype (slope). Moving 0→1→2 copies adds ≈ effect_size each step.\n",
    "  - Simulation vs real data: in this notebook we choose effect_size (β). In real studies, β is unknown and estimated (β̂) via GWAS (per‑SNP regression), with standard errors and p‑values.\n",
    "  - Larger |effect_size| generally produces bigger |r| and higher PRS R², but detectability also depends on sample size, allele frequency, and noise. In this notebook we use r as a simple proxy weight for β̂.\n",
    "\n",
    "- Quick checks\n",
    "  - Genotypes are only 0/1/2.\n",
    "  - Train and test (when viewed) should be on similar scales given the same settings.\n",
    "  - Causal SNPs won’t be visually obvious; association appears via correlation.\n",
    "\n",
    "- Next\n",
    "  - Standardize (z-score) to compare SNP–trait correlations on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ground truth effect sizes\n",
    "plt.figure(figsize=(8, 3))\n",
    "causal_idx = data['causal_snps']\n",
    "effect_sizes = beta[causal_idx]\n",
    "plt.stem(np.arange(len(causal_idx)), effect_sizes)\n",
    "plt.axhline(0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.ylabel('Effect Size (per allele)')\n",
    "plt.xlabel('Causal SNP Index')\n",
    "plt.title('True Effect Sizes (Polygenic Architecture)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c967c2c",
   "metadata": {},
   "source": [
    "### Step 2: Standardization\n",
    "\n",
    "We now convert raw genotype and phenotype values into **z-scores** in the discovery set.  A z-score answers the question: *\"How many standard deviations above (+) or below (-) the mean is this value?\"*\n",
    "\n",
    "**Formula:**  \n",
    "$z = (value - mean) / \\mathrm{SD}$\n",
    "\n",
    "**Why we do this before computing correlations:**\n",
    "- Puts every SNP column and the phenotype on the **same scale** (mean 0, SD 1).\n",
    "- Makes Pearson’s $r$ simply the **average product** of two standardized variables, which acts like an effect size.\n",
    "- Allows us to compare SNP signals fairly — rare and common variants are no longer unfairly scaled.\n",
    "- Keeps the *ordering* of individuals unchanged; we merely shift and rescale.\n",
    "\n",
    "In a polygenic context with thousands of small effects, standardization is even more important: it ensures that none of the weak signals is artificially inflated or deflated simply due to allele frequency or measurement scale differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize Genotypes and Phenotypes\n",
    "\n",
    "def standardize_matrix(mat: np.ndarray):  # -> Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "    mean = mat.mean(axis=0)\n",
    "    std = mat.std(axis=0)\n",
    "    std[std == 0] = 1.0\n",
    "    return (mat - mean) / std, mean, std\n",
    "\n",
    "Z_geno_train, geno_mean_train, geno_std_train = standardize_matrix(data['geno_train'])\n",
    "Z_geno_test, geno_mean_test_raw, geno_std_test_raw = standardize_matrix(data['geno_test'])  # independent standardization\n",
    "\n",
    "phen_train = data['phen_train']\n",
    "phen_test = data['phen_test']\n",
    "Z_phen_train = (phen_train - phen_train.mean()) / phen_train.std()\n",
    "Z_phen_test = (phen_test - phen_test.mean()) / phen_test.std()\n",
    "\n",
    "# Diagnostics to show why raw vs standardized look similar\n",
    "print(\"Raw phenotype   mean/std = {:.3f} / {:.3f}\".format(phen_train.mean(), phen_train.std()))\n",
    "print(\"Standardized    mean/std = {:.3f} / {:.3f}\".format(Z_phen_train.mean(), Z_phen_train.std()))\n",
    "print(\"Raw   min/max = {:.3f} / {:.3f}\".format(phen_train.min(), phen_train.max()))\n",
    "print(\"Z     min/max = {:.3f} / {:.3f}\".format(Z_phen_train.min(), Z_phen_train.max()))\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "# Raw distribution\n",
    "axes[0].hist(phen_train, bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(phen_train.mean(), color='k', linestyle='--', linewidth=1, label='Mean')\n",
    "axes[0].set_title('Raw Phenotype (Discovery)')\n",
    "axes[0].set_xlabel('Phenotype')\n",
    "axes[0].legend()\n",
    "\n",
    "# Standardized distribution (fixed axis to emphasize z-scale)\n",
    "axes[1].hist(Z_phen_train, bins=30, color='salmon', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].set_xlim(-4,4)\n",
    "axes[1].set_title('Standardized Phenotype (Discovery)')\n",
    "axes[1].set_xlabel('Z-Phenotype')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional viz: one SNP before/after standardization (discovery set)\n",
    "# Pick a SNP with allele frequency near 0.30 for variability\n",
    "p = data['allele_freqs']\n",
    "idx_snp = int(np.argmin(np.abs(p - 0.30)))\n",
    "x_raw = data['geno_train'][:, idx_snp]\n",
    "x_z = Z_geno_train[:, idx_snp]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "# Left: raw genotype distribution (0/1/2)\n",
    "axes[0].hist(x_raw, bins=[-0.5, 0.5, 1.5, 2.5], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xticks([0,1,2])\n",
    "axes[0].axvline(x_raw.mean(), color='k', linestyle='--', linewidth=1, label=f\"Mean={x_raw.mean():.2f}\")\n",
    "axes[0].set_title(f\"SNP_{idx_snp:04d} (raw)\")\n",
    "axes[0].set_xlabel('Genotype (0/1/2)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(frameon=False)\n",
    "\n",
    "# Right: standardized genotype distribution (z-scores)\n",
    "axes[1].hist(x_z, bins=30, color='indianred', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].axvline(1, color='gray', linestyle=':', linewidth=1, label='±1 SD')\n",
    "axes[1].axvline(-1, color='gray', linestyle=':', linewidth=1)\n",
    "axes[1].set_title(f\"SNP_{idx_snp:04d} (standardized)\")\n",
    "axes[1].set_xlabel('Z-score')\n",
    "axes[1].legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefc799",
   "metadata": {},
   "source": [
    "### Step 3: GWAS Correlations\n",
    "\n",
    "**Goal.** Quantify the relationship between each SNP and the trait.\n",
    "\n",
    "**What we do.** For each SNP $j$, we compute the Pearson correlation $r_j$ between its standardized genotypes and the standardized phenotype in the discovery set.  Because both are z-scored, $r_j$ is just the average product of two series of standardized numbers.\n",
    "\n",
    "**How to read $r$:**\n",
    "- $r_j \\approx 0$ means no detectable association.\n",
    "- $r_j > 0$ means individuals with more minor alleles tend to have **higher** trait values.\n",
    "- $r_j < 0$ means individuals with more minor alleles tend to have **lower** trait values.\n",
    "- Larger $|r_j|$ implies a stronger SNP–trait link on a common scale (and $r_j^2$ is the in-sample variance explained by SNP $j$).\n",
    "\n",
    "**Why so small?**  In a diffuse polygenic architecture, each true causal SNP has a tiny effect.  Their $|r_j|$ values are buried in sampling noise, so the largest observed $|r_j|$ will typically be modest (≈ 0.17) and may not exceed the permutation threshold.  The important information is in the **ranking** of $|r_j|$, not the individual magnitude.\n",
    "\n",
    "**Intuition.**  To decide if a SNP and trait move together, imagine multiplying their standardized values person-by-person and averaging: if the values tend to be above average together, the average product is positive; if one tends to be high when the other is low, the average product is negative; if there’s no pattern, the average product is near zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: GWAS via per-SNP correlations\n",
    "\n",
    "r_values = (Z_geno_train * Z_phen_train[:, None]).mean(axis=0)\n",
    "r_abs = np.abs(r_values)\n",
    "max_r = r_abs.max()\n",
    "print(f\"Maximum |r| = {max_r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74969cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Step 3 viz: one SNP vs phenotype (discovery set)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = int(np.argmax(r_abs))  # SNP with largest |r|\n",
    "snp_name = f\"SNP_{idx:04d}\"\n",
    "\n",
    "x_raw = data['geno_train'][:, idx]\n",
    "y_raw = phen_train\n",
    "x_z = Z_geno_train[:, idx]\n",
    "y_z = Z_phen_train\n",
    "r = float(r_values[idx]); r2 = r*r\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "jit = (rng.random(x_raw.size) - 0.5) * 0.12  # jitter to separate 0/1/2 columns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Raw scale: shows per-allele shifts\n",
    "axes[0].scatter(x_raw + jit, y_raw, s=8, alpha=0.5)\n",
    "means = [y_raw[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[0].plot([0,1,2], means, 'r-o', lw=2, label='Group means')\n",
    "axes[0].set_xticks([0,1,2]); axes[0].legend(frameon=False)\n",
    "axes[0].set_xlabel('Genotype (0/1/2)'); axes[0].set_ylabel('Phenotype (raw)')\n",
    "axes[0].set_title(f'{snp_name} vs phen_train (raw)')\n",
    "\n",
    "# Z scale: matches how r is computed\n",
    "axes[1].scatter(x_z + jit, y_z, s=8, alpha=0.5)\n",
    "# Group means in z space (plot at mean x_z for each genotype 0/1/2)\n",
    "x_means = [x_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "y_means = [y_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[1].plot(x_means, y_means, 'r-o', lw=2, label='Group means')\n",
    "axes[1].legend(frameon=False)\n",
    "axes[1].set_xlabel('Genotype (z-scored)'); axes[1].set_ylabel('Phenotype (z-scored)')\n",
    "axes[1].set_title(f'{snp_name} (|r|={abs(r):.2f}, R²={r2:.2f})')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be68a03",
   "metadata": {},
   "source": [
    "### How the scatterplot connects to Step 3 correlations - Polygenic Case\n",
    "\n",
    "- Left panel (raw):\n",
    "  - Three vertical bands (genotype 0/1/2). The red \"group means\" line shows the average phenotype for each genotype.\n",
    "  - In this polygenic architecture, even the SNP with largest |r| shows only a mild slope, reflecting its small effect size.\n",
    "  - The difference between genotype groups is modest compared to the within-group variation.\n",
    "\n",
    "- Right panel (z-scored):\n",
    "  - Both axes are standardized, which is exactly how we compute r in Step 3.\n",
    "  - Pearson r for this SNP is the average product r = mean(x_z · y_z).\n",
    "  - Notice the r value is small (typically ~0.15-0.17) - this is expected in polygenic traits where each SNP has a tiny effect.\n",
    "  - In polygenic architectures, no single SNP explains much variance (low R²).\n",
    "\n",
    "- Why this matters:\n",
    "  - Even the \"best\" SNP shows a weak association - yet collectively, many such weak signals can be informative.\n",
    "  - The value of r becomes our weight when building the polygenic score in Step 5.\n",
    "  - In polygenic traits, we rely on aggregating many weak signals rather than finding a few strong ones.\n",
    "\n",
    "Note: The cloud of points lacks the clear upward trend seen in Case 1 (sparse architecture), illustrating why polygenic traits need different analytical approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40aea0",
   "metadata": {},
   "source": [
    "### Mini-GWAS framing (what we’re mimicking)\n",
    "\n",
    "- A GWAS tests each SNP across the genome for association with a phenotype (one SNP at a time).  \n",
    "- Real GWAS uses regression (**β, SE, p-value**) and includes covariates (age, sex, ancestry PCs) to control confounding.  \n",
    "- In this exercise, we use **z-scores** and compute a simple **per-SNP correlation (r)** as a stand-in for GWAS effect size.  \n",
    "- Output of this step is a “summary stats–like” table: **SNP ID, r, |r|**.  \n",
    "- Next, we’ll visualize all SNPs together (Manhattan plot) and set a significance cut line via **permutation**—analogous to GWAS genome-wide thresholds.  \n",
    "\n",
    "**TL;DR:** Step 3 ≈ a **mini-GWAS** pass over SNPs to get per-SNP effects we can carry forward.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619476da",
   "metadata": {},
   "source": [
    "### Step 4: Permutation (simple) + Manhattan Plot\n",
    "\n",
    "From one SNP to all SNPs\n",
    "- Step 3 computed an r for each SNP; the Manhattan plot shows these |r| values across all SNPs on the same z-scale.\n",
    "\n",
    "What we do (simple permutation)\n",
    "- We ask: “How tall could the biggest |r| be just by chance?”\n",
    "- Shuffle the phenotype B times.\n",
    "- Each time, compute |r| across all SNPs and record the maximum.\n",
    "- Use the 95th percentile of these maxima as the dashed threshold line.\n",
    "\n",
    "How to read the Manhattan\n",
    "- Dot height = |r|; the dashed line is the permutation-based threshold.\n",
    "- Red stars = Top‑K SNPs by |r| (used for PRS), even if none pass the threshold.\n",
    "- In a polygenic trait, most points sit near zero; few (if any) exceed the line.\n",
    "\n",
    "Polygenic interpretation\n",
    "- Lack of peaks above the threshold does not mean “no genetics.”\n",
    "- The useful signal is in the ranking of many small effects; we aggregate Top‑K into a PRS in Step 5.\n",
    "\n",
    "Beginner tip\n",
    "- Think “many pennies add up,” or a “tallest building contest”: the line sits just above how tall the tallest would be if there were no real signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "# Step 4: Compact permutation + Manhattan (simple line + Top‑K overlay)\n",
    "def perm_threshold(Z_geno: np.ndarray, y: np.ndarray, B: int = 100, q: float = 0.95, seed: int = 0) -> float:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_abs = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        y_shuf = rng.permutation(y)\n",
    "        r_shuf = (Z_geno * y_shuf[:, None]).mean(axis=0)\n",
    "        max_abs[b] = np.abs(r_shuf).max()\n",
    "    return float(np.quantile(max_abs, q))\n",
    "\n",
    "ALPHA = 0.05\n",
    "B = config.n_permutations if hasattr(config, \"n_permutations\") else 100\n",
    "threshold = perm_threshold(Z_geno_train, Z_phen_train, B=B, q=1-ALPHA, seed=0)\n",
    "\n",
    "# Selection for display and PRS\n",
    "r_abs = np.abs(r_values)\n",
    "topK = config.broad_PGS_k\n",
    "topK_idx = np.argsort(r_abs)[-topK:]\n",
    "selected_idx = np.flatnonzero(r_abs >= threshold)  # often 0 in polygenic case\n",
    "\n",
    "print(f\"Permutation threshold (95%): {threshold:.3f} | Above threshold: {selected_idx.size} | Top-K (PRS) = {topK}\")\n",
    "\n",
    "# Manhattan plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(config.num_snps)\n",
    "plt.scatter(x, r_abs, s=10, c='black', alpha=0.6, label='All SNPs')\n",
    "plt.scatter(topK_idx, r_abs[topK_idx], s=40, c='red', alpha=0.9, marker='*', label=f'Top {topK} by |r| (PRS)')\n",
    "plt.axhline(threshold, color='tab:blue', linestyle='--', linewidth=1.5, label='95% permutation threshold')\n",
    "plt.xlabel('SNP index'); plt.ylabel('Absolute correlation |r|')\n",
    "plt.title('Manhattan Plot (Permutation line + Top‑K overlay)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7853a2",
   "metadata": {},
   "source": [
    "### From GWAS results to Manhattan (our version)\n",
    "\n",
    "- Classic GWAS Manhattan plots **−log10(p)** by genomic position; taller peaks = stronger evidence.  \n",
    "- Here, we plot **|r|** instead of −log10(p). The goal is the same: a genome-wide view of signal strength.  \n",
    "- GWAS uses fixed significance lines (e.g., *5×10⁻⁸*). We use a **permutation-based line** that reflects our data.  \n",
    "- Selection rule: take all SNPs above the line; if none cross, use a **top-K by |r| fallback** (keep the sign of r for direction).  \n",
    "- Same idea, simpler ingredients: our permutation line plays the role of a **GWAS significance threshold**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace0bc6",
   "metadata": {},
   "source": [
    "### Step 5: Build and Evaluate a Polygenic Score\n",
    "\n",
    "**Goal:** Aggregate many weak genetic signals into a single predictive score and evaluate its performance.\n",
    "\n",
    "**What we do:**\n",
    "1. **Select SNPs:** Since no SNPs pass the permutation threshold, we use the top-K SNPs by |r| (K=50)\n",
    "2. **Calculate PRS:** Weighted sum of standardized genotypes (weights = correlations from discovery)\n",
    "3. **Evaluate performance:** Correlation metrics (r, R²) and decile stratification\n",
    "4. **Visualize:** Scatter plot and decile plot showing PRS-phenotype relationship\n",
    "\n",
    "In polygenic traits, the decile plot is especially valuable - it demonstrates that even when no individual SNP is significant, aggregating many weak signals can still stratify individuals by genetic risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build and evaluate polygenic score (PRS)\n",
    "\n",
    "# Always use top-K SNPs (no genome-wide hits)\n",
    "selected_idx = topK_idx\n",
    "label = f'Top-{topK} by |r|'\n",
    "\n",
    "# Build raw PRS for target samples\n",
    "prs_raw = Z_geno_test[:, selected_idx] @ r_values[selected_idx]\n",
    "# Standardize PRS\n",
    "prs = (prs_raw - prs_raw.mean()) / prs_raw.std()\n",
    "\n",
    "# Correlation with phenotype\n",
    "R = float(np.corrcoef(prs, Z_phen_test)[0,1]) if prs.std() > 0 else 0.0\n",
    "R2 = R * R\n",
    "print(f\"PRS type: {label}\")\n",
    "print(f\"SNPs used: {len(selected_idx)} | Pearson r = {R:.3f} | R^2 = {R2:.3f}\")\n",
    "\n",
    "# Visualization: Scatter plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(prs, Z_phen_test, s=8, alpha=0.4)\n",
    "plt.xlabel('PRS (z-score)')\n",
    "plt.ylabel('Phenotype (z-score)')\n",
    "plt.title(f'PRS vs Phenotype (r={R:.2f}, R²={R2:.2f})')\n",
    "\n",
    "# Visualization: Decile plot\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create deciles\n",
    "edges = np.quantile(prs, np.linspace(0,1,11))\n",
    "dec = np.digitize(prs, edges[1:-1], right=True)\n",
    "mean_per_decile = [Z_phen_test[dec == d].mean() if np.sum(dec==d) > 0 else np.nan for d in range(10)]\n",
    "se_per_decile = [Z_phen_test[dec == d].std(ddof=1)/np.sqrt(np.sum(dec==d)) if np.sum(dec==d) > 1 else np.nan for d in range(10)]\n",
    "gap = mean_per_decile[-1] - mean_per_decile[0] if not np.isnan(mean_per_decile[0]) and not np.isnan(mean_per_decile[-1]) else np.nan\n",
    "\n",
    "plt.errorbar(range(1,11), mean_per_decile, yerr=se_per_decile, fmt='-o', capsize=3)\n",
    "plt.title(f'Phenotype by PRS Decile (Δ₁₀–₁ ≈ {gap:.2f} SD)')\n",
    "plt.xlabel('PRS Decile (1=lowest, 10=highest)')\n",
    "plt.ylabel('Mean Phenotype (z-score)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5654d",
   "metadata": {},
   "source": [
    "### Step 5 Interpretation\n",
    "\n",
    "**Performance metrics:**\n",
    "- PRS type: Top-50 by |r|\n",
    "- SNPs used: 50\n",
    "- Correlation: r ≈ 0.16, R² ≈ 0.02\n",
    "\n",
    "**What this means:**\n",
    "- **No genome-wide significant SNPs:** In this polygenic architecture, no single variant has a large enough effect to pass the threshold\n",
    "- **Modest but meaningful prediction:** Despite small individual effects, aggregating the top 50 SNPs produces a score that correlates with the trait\n",
    "- **Small R²:** The PRS explains only a small fraction of variance, reflecting the challenging nature of polygenic prediction\n",
    "\n",
    "**Understanding the decile plot:**\n",
    "- Each point shows the average trait value for individuals in that PRS decile\n",
    "- Error bars represent standard error of the mean\n",
    "- The gradual upward slope confirms the PRS has real predictive value\n",
    "- The gap between lowest and highest deciles (Δ₁₀-₁ ≈ 0.5 SD) represents the practical effect size\n",
    "- In this polygenic case, the gradient is more modest than in a sparse architecture, but still shows clear stratification\n",
    "\n",
    "**Why this matters:**\n",
    "This demonstrates a key principle of complex traits: even when no single variant reaches significance, the aggregated small effects can still provide useful prediction. Most human traits and diseases follow this pattern, where many tiny genetic effects combine to influence outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbb4ed",
   "metadata": {},
   "source": [
    "### Conclusion: What We Learned from Case 2\n",
    "\n",
    "**Journey summary**  \n",
    "We explored a diffuse polygenic architecture where many SNPs each have tiny effects. No single SNP clears a strict threshold, yet aggregating weak signals into a PRS yields measurable prediction.\n",
    "\n",
    "**What we accomplished**\n",
    "1. Simulated a polygenic trait (h² ≈ 0.30) with many small-effect causal SNPs\n",
    "2. Standardized genotypes and phenotypes for fair, comparable correlations\n",
    "3. Computed per-SNP r values and used permutations to set a family-wise threshold\n",
    "4. Observed a “flat sea” Manhattan plot with no genome-wide hits (as expected)\n",
    "5. Built a PRS from the top-K SNPs by |r| and evaluated it on a held-out set\n",
    "6. Visualized performance via scatter and decile plots (modest r, small R², clear but gradual stratification)\n",
    "\n",
    "**Key insights**\n",
    "- In polygenic traits, the useful signal lies in the ranking of many small effects; aggregation beats single-marker significance\n",
    "- Lack of significant peaks does not imply a non-genetic trait\n",
    "- PRS performance is modest per individual but informative at the group level; it improves with larger training N and better methods\n",
    "- Standardization and honest holdout evaluation prevent leakage and overstatement of accuracy\n",
    "\n",
    "**Why this matters**  \n",
    "Most complex traits and common diseases are polygenic. Even modest PRS can stratify risk and inform research, screening, and trial enrichment when used responsibly.\n",
    "\n",
    "**Taking it further**\n",
    "- Tune K via cross-validation; add LD-aware/shrinkage methods (clump+threshold, ridge/BLUP, LDpred, PRS-CS, lassosum)\n",
    "- Increase discovery sample size or use external GWAS summary stats\n",
    "- Include covariates (age/sex/PCs), check ancestry transferability, and assess calibration/clinical utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0e919",
   "metadata": {},
   "source": [
    "# Complete the Reflection & Comparison Questions in Shared Slides in Groups in Canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
