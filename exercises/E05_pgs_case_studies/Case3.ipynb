{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d03bf6",
   "metadata": {},
   "source": [
    "# Case 3: Null GWAS (No True Associations)\n",
    "\n",
    "This notebook follows the GWAS→PRS workflow but simulates a trait with no genetic basis at all—a true null. No SNP truly affects the trait; the phenotype is just random noise, unrelated to genotype. This scenario serves as a negative control, showing what GWAS and PRS look like when there is zero real signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e8b8b",
   "metadata": {},
   "source": [
    "### Step 0: Imports\n",
    "\n",
    "Import required libraries for simulation, computation and plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730903ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display\n",
    "\n",
    "# Style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46069ec9",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Genotypes & Phenotypes (Null Trait)\n",
    "\n",
    "This case simulates a trait with no genetic basis at all. We split individuals 70/30 into discovery (training) and target (testing) sets, but unlike Cases 1 and 2, the phenotype is pure noise with no genetic component.\n",
    "\n",
    "**Configuration (dataclass):**\n",
    "We define all simulation knobs once:\n",
    "\n",
    "- num_snps: total SNPs (default 1000)\n",
    "- num_individuals: total people (split 70/30 into discovery/target)\n",
    "- num_causal: set to 0 - no SNPs affect the trait\n",
    "- effect_size: set to 0.00 - no genetic effects\n",
    "- n_permutations: number of shuffles to build the empirical max |r| threshold (used in Step 4)\n",
    "- broad_PGS_k: how many top-|r| SNPs to aggregate into the PRS (used in Step 5)\n",
    "- noise_std: standard deviation of phenotype (pure noise)\n",
    "\n",
    "(You can change these in the SimulationConfig block below and re‑run.)\n",
    "\n",
    "**What are we making?**\n",
    "A control dataset for learning about random associations. Each \"person\" gets:\n",
    "- Genotypes: 0 / 1 / 2 copies of an allele at 1,000 SNPs\n",
    "- Phenotype: Pure random noise with no relation to genotype\n",
    "\n",
    "**Why two groups?**\n",
    "We discover patterns on one set and test them fairly on a fresh set.\n",
    "- Discovery set (train): estimate SNP–trait associations (all will be random)\n",
    "- Target set (test): honest evaluation showing no true predictive power\n",
    "\n",
    "**How do we simulate?**\n",
    "1) Draw an allele frequency for each SNP (uniform between 0.05 and 0.5).  \n",
    "2) Generate genotypes (0/1/2) via binomial draws given each SNP's allele frequency.  \n",
    "3) Generate phenotypes as pure random normal noise, completely independent of genotypes.  \n",
    "4) Split data into discovery and target sets for evaluation.\n",
    "\n",
    "Note: In a null architecture with h² = 0, no SNP should clear a significance threshold except by random chance. Any apparent association is a false positive, and any PRS should have no predictive power beyond random fluctuation.\n",
    "\n",
    "**Beginner prompts (try these for deeper understanding):**\n",
    "- \"What does a truly random Manhattan plot look like?\"\n",
    "- \"How high can correlations get by pure chance?\"\n",
    "- \"Why do we still see non-zero correlations when there's no true effect?\"\n",
    "- \"How does this null case help us interpret results from Cases 1 and 2?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Config + Null Trait Simulation + Train Preview\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "@dataclass\n",
    "class SimulationConfig:\n",
    "    num_snps: int = 1000\n",
    "    num_individuals: int = 4000\n",
    "    num_causal: int = 0  # No causal SNPs\n",
    "    effect_size: float = 0.00  # No effect\n",
    "    noise_std: float = 1.0\n",
    "    n_permutations: int = 100\n",
    "    broad_PGS_k: int = 50\n",
    "\n",
    "config = SimulationConfig()\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# 1. Draw allele frequencies and genotypes (0/1/2)\n",
    "allele_freqs = rng.uniform(0.05, 0.5, size=config.num_snps)\n",
    "geno = np.empty((config.num_individuals, config.num_snps), dtype=np.int8)\n",
    "for j, p in enumerate(allele_freqs):\n",
    "    geno[:, j] = rng.binomial(2, p, size=config.num_individuals)\n",
    "\n",
    "# 2. Split into discovery (train) and target (test) sets\n",
    "n_train = int(0.7 * config.num_individuals)\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, config.num_individuals)\n",
    "geno_train, geno_test = geno[train_idx], geno[test_idx]\n",
    "\n",
    "# 3. No causal SNPs - empty list for consistency\n",
    "causal_idx = np.array([], dtype=int)\n",
    "\n",
    "# 4. No genetic component - pure environmental phenotype\n",
    "phen_train = rng.normal(0.0, 1.0, size=n_train)\n",
    "phen_test = rng.normal(0.0, 1.0, size=config.num_individuals - n_train)\n",
    "\n",
    "# Package for later steps\n",
    "data = {\n",
    "    \"allele_freqs\": allele_freqs,\n",
    "    \"geno_train\": geno_train,\n",
    "    \"geno_test\": geno_test,\n",
    "    \"phen_train\": phen_train,\n",
    "    \"phen_test\": phen_test,\n",
    "    \"causal_snps\": causal_idx,\n",
    "}\n",
    "\n",
    "# Diagnostics\n",
    "print(f\"Train phenotype mean/var: {phen_train.mean():.3f} / {phen_train.var():.3f}\")\n",
    "print(f\"Empirical h^2 = 0.000 (null trait)\")\n",
    "print(f\"Number of causal SNPs: {len(causal_idx)}\")\n",
    "\n",
    "# Preview first 5 rows\n",
    "preview_snps = 8\n",
    "df_train = pd.DataFrame(geno_train[:5, :preview_snps],\n",
    "                        columns=[f\"SNP_{i:04d}\" for i in range(preview_snps)])\n",
    "df_train[\"phen_train\"] = phen_train[:5]\n",
    "print(\"Train (wide) first rows:\")\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff7ea6",
   "metadata": {},
   "source": [
    "### Interpreting the Training Set Preview (Null Trait)\n",
    "\n",
    "You're seeing only the discovery (training) table. The target (test) data exists but isn't shown here.\n",
    "\n",
    "- Columns\n",
    "  - SNP_0000, SNP_0001, …: Genotypes coded 0/1/2 (0=none, 1=heterozygous, 2=homozygous effect).\n",
    "  - phen_train: Simulated trait = pure random noise, completely unrelated to genotype. Higher values are not associated with any genetic pattern.\n",
    "\n",
    "- Reading a row (example)\n",
    "  - SNP_0000=1, SNP_0001=2, …, phen_train=3.47 → This phenotype value is random noise, with no relationship to the genotypes.\n",
    "\n",
    "- effect_size (what it means)\n",
    "  - Per-allele additive change in the phenotype (slope). Moving 0→1→2 copies adds ≈ effect_size each step.\n",
    "  - Simulation vs real data: in this notebook we choose effect_size (β). In real studies, β is unknown and estimated (β̂) via GWAS (per‑SNP regression), with standard errors and p‑values.\n",
    "  - Larger |effect_size| generally produces bigger |r| and higher PRS R², but detectability also depends on sample size, allele frequency, and noise. In this notebook we use r as a simple proxy weight for β̂.\n",
    "  \n",
    "- Key points\n",
    "  - No SNPs are causal - the trait has zero heritability (h² = 0)\n",
    "  - Any observed SNP-trait associations are purely due to random chance\n",
    "  - Train and test are separate draws; don't use test data until evaluation.\n",
    "  - In this null case, we expect a completely flat Manhattan plot with no significant hits.\n",
    "\n",
    "- Quick checks\n",
    "  - Genotypes are only 0/1/2.\n",
    "  - Train and test phenotypes are independent draws from standard normal distributions.\n",
    "  - No correlation between genotype and phenotype should persist in the test set.\n",
    "\n",
    "- Next\n",
    "  - Standardize (z-score) to compare SNP–trait correlations on a common scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c967c2c",
   "metadata": {},
   "source": [
    "### Step 3: GWAS Correlations (Null Trait)\n",
    "\n",
    "**Goal:** Quantify the relationship between each SNP and the trait when no true association exists.\n",
    "\n",
    "**How we compute:** With z-scores, the per-SNP Pearson correlation is the average product:\n",
    "  \n",
    "$$\n",
    "r_j = \\frac{1}{N}\\sum_{i=1}^{N} G^{(z)}_{ij}\\,y^{(z)}_i\n",
    "$$\n",
    "\n",
    "Where N = number of individuals, G^{(z)}_{ij} = standardized genotype at SNP j, y^{(z)}_i = standardized phenotype.\n",
    "\n",
    "**What to expect:** \n",
    "- All correlations result from random chance\n",
    "- r_j values will be small and randomly distributed around zero\n",
    "- The largest |r| values are just statistical flukes\n",
    "\n",
    "**Why this matters:** Understanding the distribution of correlations under the null hypothesis helps us recognize when observed associations in real data are meaningful versus when they could arise by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize Genotypes and Phenotypes\n",
    "\n",
    "def standardize_matrix(mat: np.ndarray):  # -> Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "    mean = mat.mean(axis=0)\n",
    "    std = mat.std(axis=0)\n",
    "    std[std == 0] = 1.0\n",
    "    return (mat - mean) / std, mean, std\n",
    "\n",
    "Z_geno_train, geno_mean_train, geno_std_train = standardize_matrix(data['geno_train'])\n",
    "Z_geno_test, geno_mean_test_raw, geno_std_test_raw = standardize_matrix(data['geno_test'])  # independent standardization\n",
    "\n",
    "phen_train = data['phen_train']\n",
    "phen_test = data['phen_test']\n",
    "Z_phen_train = (phen_train - phen_train.mean()) / phen_train.std()\n",
    "Z_phen_test = (phen_test - phen_test.mean()) / phen_test.std()\n",
    "\n",
    "# Diagnostics to show why raw vs standardized look similar\n",
    "print(\"Raw phenotype   mean/std = {:.3f} / {:.3f}\".format(phen_train.mean(), phen_train.std()))\n",
    "print(\"Standardized    mean/std = {:.3f} / {:.3f}\".format(Z_phen_train.mean(), Z_phen_train.std()))\n",
    "print(\"Raw   min/max = {:.3f} / {:.3f}\".format(phen_train.min(), phen_train.max()))\n",
    "print(\"Z     min/max = {:.3f} / {:.3f}\".format(Z_phen_train.min(), Z_phen_train.max()))\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "# Raw distribution\n",
    "axes[0].hist(phen_train, bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(phen_train.mean(), color='k', linestyle='--', linewidth=1, label='Mean')\n",
    "axes[0].set_title('Raw Phenotype (Discovery)')\n",
    "axes[0].set_xlabel('Phenotype')\n",
    "axes[0].legend()\n",
    "\n",
    "# Standardized distribution (fixed axis to emphasize z-scale)\n",
    "axes[1].hist(Z_phen_train, bins=30, color='salmon', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].set_xlim(-4,4)\n",
    "axes[1].set_title('Standardized Phenotype (Discovery)')\n",
    "axes[1].set_xlabel('Z-Phenotype')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c76d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional viz: one SNP before/after standardization (discovery set, null trait)\n",
    "# Pick a SNP with allele frequency near 0.30 for variability\n",
    "p = data['allele_freqs']\n",
    "idx_snp = int(np.argmin(np.abs(p - 0.30)))\n",
    "x_raw = data['geno_train'][:, idx_snp]\n",
    "x_z = Z_geno_train[:, idx_snp]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "# Left: raw genotype distribution (0/1/2)\n",
    "axes[0].hist(x_raw, bins=[-0.5, 0.5, 1.5, 2.5], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xticks([0, 1, 2])\n",
    "axes[0].axvline(x_raw.mean(), color='k', linestyle='--', linewidth=1, label=f\"Mean={x_raw.mean():.2f}\")\n",
    "axes[0].set_title(f\"SNP_{idx_snp:04d} (raw)\")\n",
    "axes[0].set_xlabel('Genotype (0/1/2)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(frameon=False)\n",
    "\n",
    "# Right: standardized genotype distribution (z-scores)\n",
    "axes[1].hist(x_z, bins=30, color='indianred', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].axvline(1, color='gray', linestyle=':', linewidth=1, label='±1 SD')\n",
    "axes[1].axvline(-1, color='gray', linestyle=':', linewidth=1)\n",
    "axes[1].set_title(f\"SNP_{idx_snp:04d} (standardized)\")\n",
    "axes[1].set_xlabel('Z-score')\n",
    "axes[1].legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefc799",
   "metadata": {},
   "source": [
    "### Step 3: GWAS Correlations\n",
    "\n",
    "**Goal.** Quantify the relationship between each SNP and the trait.\n",
    "\n",
    "**What we do.** For each SNP $j$, we compute the Pearson correlation $r_j$ between its standardized genotypes and the standardized phenotype in the discovery set.  Because both are z-scored, $r_j$ is just the average product of two series of standardized numbers.\n",
    "\n",
    "**How to read $r$:**\n",
    "- $r_j \\approx 0$ means no detectable association.\n",
    "- $r_j > 0$ means individuals with more minor alleles tend to have **higher** trait values.\n",
    "- $r_j < 0$ means individuals with more minor alleles tend to have **lower** trait values.\n",
    "- Larger $|r_j|$ implies a stronger SNP–trait link on a common scale (and $r_j^2$ is the in-sample variance explained by SNP $j$).\n",
    "\n",
    "**Why so small?**  In a diffuse polygenic architecture, each true causal SNP has a tiny effect.  Their $|r_j|$ values are buried in sampling noise, so the largest observed $|r_j|$ will typically be modest (≈ 0.17) and may not exceed the permutation threshold.  The important information is in the **ranking** of $|r_j|$, not the individual magnitude.\n",
    "\n",
    "**Intuition.**  To decide if a SNP and trait move together, imagine multiplying their standardized values person-by-person and averaging: if the values tend to be above average together, the average product is positive; if one tends to be high when the other is low, the average product is negative; if there’s no pattern, the average product is near zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: GWAS via per-SNP correlations\n",
    "\n",
    "r_values = (Z_geno_train * Z_phen_train[:, None]).mean(axis=0)\n",
    "r_abs = np.abs(r_values)\n",
    "max_r = r_abs.max()\n",
    "print(f\"Maximum |r| = {max_r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Step 3 viz: one SNP vs phenotype (discovery set)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = int(np.argmax(r_abs))  # SNP with largest |r|\n",
    "snp_name = f\"SNP_{idx:04d}\"\n",
    "\n",
    "x_raw = data['geno_train'][:, idx]\n",
    "y_raw = phen_train\n",
    "x_z = Z_geno_train[:, idx]\n",
    "y_z = Z_phen_train\n",
    "r = float(r_values[idx]); r2 = r*r\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "jit = (rng.random(x_raw.size) - 0.5) * 0.12  # jitter to separate 0/1/2 columns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Raw scale: shows per-allele shifts\n",
    "axes[0].scatter(x_raw + jit, y_raw, s=8, alpha=0.5)\n",
    "means = [y_raw[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[0].plot([0,1,2], means, 'r-o', lw=2, label='Group means')\n",
    "axes[0].set_xticks([0,1,2]); axes[0].legend(frameon=False)\n",
    "axes[0].set_xlabel('Genotype (0/1/2)'); axes[0].set_ylabel('Phenotype (raw)')\n",
    "axes[0].set_title(f'{snp_name} vs phen_train (raw)')\n",
    "\n",
    "# Z scale: matches how r is computed\n",
    "axes[1].scatter(x_z + jit, y_z, s=8, alpha=0.5)\n",
    "# Group means in z space (plot at mean x_z for each genotype 0/1/2)\n",
    "x_means = [x_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "y_means = [y_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[1].plot(x_means, y_means, 'r-o', lw=2, label='Group means')\n",
    "axes[1].legend(frameon=False)\n",
    "axes[1].set_xlabel('Genotype (z-scored)'); axes[1].set_ylabel('Phenotype (z-scored)')\n",
    "axes[1].set_title(f'{snp_name} (|r|={abs(r):.2f}, R²={r2:.2f})')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1fa0d",
   "metadata": {},
   "source": [
    "### How the scatterplot connects to Step 3 correlations - Null Case\n",
    "\n",
    "- Left panel (raw):\n",
    "  - Three vertical bands (genotype 0/1/2). The red \"group means\" line shows the average phenotype for each genotype.\n",
    "  - In this null case, any apparent pattern is pure chance - the phenotype is random noise unrelated to genotype.\n",
    "  - Despite this, we still see some variation in group means purely by random sampling.\n",
    "\n",
    "- Right panel (z-scored):\n",
    "  - Both axes are standardized, which is exactly how we compute r in Step 3.\n",
    "  - Pearson r for this SNP is the average product r = mean(x_z · y_z).\n",
    "  - The non-zero r value (typically ~0.10-0.15) is a false positive - it arose purely by chance.\n",
    "  - R² shows how much variance this SNP \"explains\" in-sample, but this won't replicate in new data.\n",
    "\n",
    "- Why this matters:\n",
    "  - This null case demonstrates why we need statistical thresholds - random correlations happen by chance.\n",
    "  - The permutation threshold (Step 4) helps distinguish real signals from random noise.\n",
    "  - Even with no true genetic effects, some SNPs will appear \"promising\" in the discovery set.\n",
    "  - The Manhattan plot will show random fluctuations around zero with no systematic pattern.\n",
    "\n",
    "Note: This null case provides an important baseline for understanding what random associations look like, helping us interpret the results in the other cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecacf95",
   "metadata": {},
   "source": [
    "### Mini-GWAS framing (what we’re mimicking)\n",
    "\n",
    "- A GWAS tests each SNP across the genome for association with a phenotype (one SNP at a time).  \n",
    "- Real GWAS uses regression (**β, SE, p-value**) and includes covariates (age, sex, ancestry PCs) to control confounding.  \n",
    "- In this exercise, we use **z-scores** and compute a simple **per-SNP correlation (r)** as a stand-in for GWAS effect size.  \n",
    "- Output of this step is a “summary stats–like” table: **SNP ID, r, |r|**.  \n",
    "- Next, we’ll visualize all SNPs together (Manhattan plot) and set a significance cut line via **permutation**—analogous to GWAS genome-wide thresholds.  \n",
    "\n",
    "**TL;DR:** Step 3 ≈ a **mini-GWAS** pass over SNPs to get per-SNP effects we can carry forward.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619476da",
   "metadata": {},
   "source": [
    "### Step 4: Permutation (simple) + Manhattan Plot (Null)\n",
    "\n",
    "From one SNP to all SNPs\n",
    "- Step 3 computed an r for each SNP on z-scored data; the Manhattan plot stacks their |r| genome‑wide on a common scale.\n",
    "\n",
    "What we do (simple permutation)\n",
    "- We ask: “How tall could the biggest |r| be just by chance?”\n",
    "- Shuffle the phenotype B times.\n",
    "- Each time, compute |r| across all SNPs and record the maximum.\n",
    "- Use the 95th percentile of these maxima as the blue dashed threshold line.\n",
    "\n",
    "How to read the Manhattan (null case)\n",
    "- Dot height = |r|; the dashed line is the permutation-based threshold.\n",
    "- Red stars = Top‑K by |r| (illustration only—not significant in a true null).\n",
    "- In a true null, points hover near zero; rare tall dots are random flukes.\n",
    "\n",
    "Interpretation under the null\n",
    "- Peaks near/above the line are expected from randomness and won’t replicate.\n",
    "- The line quantifies “how tall the tallest gets by luck.” Consistent exceedance would suggest real signal (not present here).\n",
    "- Top‑K are carried to Step 5 only to show a PRS built from noise does not generalize.\n",
    "\n",
    "Beginner tip\n",
    "- Like coin-flip streaks: some streaks appear by chance. The permutation line shows how long a streak we expect with no real effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compact permutation + Manhattan (null: simple line + Top‑K overlay)\n",
    "def perm_threshold(Z_geno: np.ndarray, y: np.ndarray, B: int = 100, q: float = 0.95, seed: int = 0) -> float:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_abs = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        y_shuf = rng.permutation(y)\n",
    "        r_shuf = (Z_geno * y_shuf[:, None]).mean(axis=0)\n",
    "        max_abs[b] = np.abs(r_shuf).max()\n",
    "    return float(np.quantile(max_abs, q))\n",
    "\n",
    "ALPHA = 0.05\n",
    "B = config.n_permutations if hasattr(config, \"n_permutations\") else 100\n",
    "threshold = perm_threshold(Z_geno_train, Z_phen_train, B=B, q=1-ALPHA, seed=0)\n",
    "\n",
    "# Display selection (illustrative Top‑K) and counts\n",
    "r_abs = np.abs(r_values)\n",
    "topK = config.broad_PGS_k\n",
    "topK_idx = np.argsort(r_abs)[-topK:]\n",
    "selected_idx = np.flatnonzero(r_abs >= threshold)  # expected ~0 in null\n",
    "\n",
    "print(f\"Permutation threshold (95%): {threshold:.3f} | Above threshold: {selected_idx.size} | Top-K (illustration) = {topK}\")\n",
    "\n",
    "# Manhattan plot (null)\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(config.num_snps)\n",
    "plt.scatter(x, r_abs, s=10, c='black', alpha=0.6, label='All SNPs')\n",
    "plt.scatter(topK_idx, r_abs[topK_idx], s=40, c='red', alpha=0.9, marker='*', label=f'Top {topK} by |r| (illustration)')\n",
    "plt.axhline(threshold, color='tab:blue', linestyle='--', linewidth=1.5, label='95% permutation threshold')\n",
    "plt.xlabel('SNP index'); plt.ylabel('Absolute correlation |r|')\n",
    "plt.title('Manhattan Plot (Null: permutation line + Top‑K overlay)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735e5eb",
   "metadata": {},
   "source": [
    "### From GWAS results to Manhattan (our version)\n",
    "\n",
    "- Classic GWAS Manhattan plots **−log10(p)** by genomic position; taller peaks = stronger evidence.  \n",
    "- Here, we plot **|r|** instead of −log10(p). The goal is the same: a genome-wide view of signal strength.  \n",
    "- GWAS uses fixed significance lines (e.g., *5×10⁻⁸*). We use a **permutation-based line** that reflects our data.  \n",
    "- Selection rule: take all SNPs above the line; if none cross, use a **top-K by |r| fallback** (keep the sign of r for direction).  \n",
    "- Same idea, simpler ingredients: our permutation line plays the role of a **GWAS significance threshold**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace0bc6",
   "metadata": {},
   "source": [
    "### Step 5: Build and Evaluate a Polygenic Score\n",
    "\n",
    "**Goal:** Aggregate many weak genetic signals into a single predictive score and evaluate its performance.\n",
    "\n",
    "**What we do:**\n",
    "1. **Select SNPs:** Since no SNPs pass the permutation threshold, we use the top-K SNPs by |r| (K=50)\n",
    "2. **Calculate PRS:** Weighted sum of standardized genotypes (weights = correlations from discovery)\n",
    "3. **Evaluate performance:** Correlation metrics (r, R²) and decile stratification\n",
    "4. **Visualize:** Scatter plot and decile plot showing PRS-phenotype relationship\n",
    "\n",
    "In polygenic traits, the decile plot is especially valuable - it demonstrates that even when no individual SNP is significant, aggregating many weak signals can still stratify individuals by genetic risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build and evaluate polygenic score (PRS)\n",
    "\n",
    "# Always use top-K SNPs (no genome-wide hits)\n",
    "selected_idx = topK_idx\n",
    "label = f'Top-{topK} by |r|'\n",
    "\n",
    "# Build raw PRS for target samples\n",
    "prs_raw = Z_geno_test[:, selected_idx] @ r_values[selected_idx]\n",
    "# Standardize PRS\n",
    "prs = (prs_raw - prs_raw.mean()) / prs_raw.std()\n",
    "\n",
    "# Correlation with phenotype\n",
    "R = float(np.corrcoef(prs, Z_phen_test)[0,1]) if prs.std() > 0 else 0.0\n",
    "R2 = R * R\n",
    "print(f\"PRS type: {label}\")\n",
    "print(f\"SNPs used: {len(selected_idx)} | Pearson r = {R:.3f} | R^2 = {R2:.3f}\")\n",
    "\n",
    "# Visualization: Scatter plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(prs, Z_phen_test, s=8, alpha=0.4)\n",
    "plt.xlabel('PRS (z-score)')\n",
    "plt.ylabel('Phenotype (z-score)')\n",
    "plt.title(f'PRS vs Phenotype (r={R:.2f}, R²={R2:.2f})')\n",
    "\n",
    "# Visualization: Decile plot\n",
    "plt.subplot(1, 2, 2)\n",
    "# Create deciles\n",
    "edges = np.quantile(prs, np.linspace(0,1,11))\n",
    "dec = np.digitize(prs, edges[1:-1], right=True)\n",
    "mean_per_decile = [Z_phen_test[dec == d].mean() if np.sum(dec==d) > 0 else np.nan for d in range(10)]\n",
    "se_per_decile = [Z_phen_test[dec == d].std(ddof=1)/np.sqrt(np.sum(dec==d)) if np.sum(dec==d) > 1 else np.nan for d in range(10)]\n",
    "gap = mean_per_decile[-1] - mean_per_decile[0] if not np.isnan(mean_per_decile[0]) and not np.isnan(mean_per_decile[-1]) else np.nan\n",
    "\n",
    "plt.errorbar(range(1,11), mean_per_decile, yerr=se_per_decile, fmt='-o', capsize=3)\n",
    "plt.title(f'Phenotype by PRS Decile (Δ₁₀–₁ ≈ {gap:.2f} SD)')\n",
    "plt.xlabel('PRS Decile (1=lowest, 10=highest)')\n",
    "plt.ylabel('Mean Phenotype (z-score)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5654d",
   "metadata": {},
   "source": [
    "### Step 5 Interpretation\n",
    "\n",
    "**Performance metrics:**\n",
    "- PRS type: Top-50 by |r|\n",
    "- SNPs used: 50\n",
    "- Correlation: r ≈ 0.16, R² ≈ 0.02\n",
    "\n",
    "**What this means:**\n",
    "- **No genome-wide significant SNPs:** In this polygenic architecture, no single variant has a large enough effect to pass the threshold\n",
    "- **Modest but meaningful prediction:** Despite small individual effects, aggregating the top 50 SNPs produces a score that correlates with the trait\n",
    "- **Small R²:** The PRS explains only a small fraction of variance, reflecting the challenging nature of polygenic prediction\n",
    "\n",
    "**Understanding the decile plot:**\n",
    "- Each point shows the average trait value for individuals in that PRS decile\n",
    "- Error bars represent standard error of the mean\n",
    "- The gradual upward slope confirms the PRS has real predictive value\n",
    "- The gap between lowest and highest deciles (Δ₁₀-₁ ≈ 0.5 SD) represents the practical effect size\n",
    "- In this polygenic case, the gradient is more modest than in a sparse architecture, but still shows clear stratification\n",
    "\n",
    "**Why this matters:**\n",
    "This demonstrates a key principle of complex traits: even when no single variant reaches significance, the aggregated small effects can still provide useful prediction. Most human traits and diseases follow this pattern, where many tiny genetic effects combine to influence outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5f362",
   "metadata": {},
   "source": [
    "**Comparison to Other Cases**\n",
    "- Unlike all other cases, there is no true genetic signal (h² = 0)\n",
    "- Any apparent associations or PRS performance is purely due to sampling variance\n",
    "- Serves as an important control to help interpret results in Cases 1, 2, and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbb4ed",
   "metadata": {},
   "source": [
    "### Conclusion: What We Learned from Case 2\n",
    "\n",
    "**Journey summary**  \n",
    "We explored a diffuse polygenic architecture where many SNPs each have tiny effects. No single SNP clears a strict threshold, yet aggregating weak signals into a PRS yields measurable prediction.\n",
    "\n",
    "**What we accomplished**\n",
    "1. Simulated a polygenic trait (h² ≈ 0.30) with many small-effect causal SNPs\n",
    "2. Standardized genotypes and phenotypes for fair, comparable correlations\n",
    "3. Computed per-SNP r values and used permutations to set a family-wise threshold\n",
    "4. Observed a “flat sea” Manhattan plot with no genome-wide hits (as expected)\n",
    "5. Built a PRS from the top-K SNPs by |r| and evaluated it on a held-out set\n",
    "6. Visualized performance via scatter and decile plots (modest r, small R², clear but gradual stratification)\n",
    "\n",
    "**Key insights**\n",
    "- In polygenic traits, the useful signal lies in the ranking of many small effects; aggregation beats single-marker significance\n",
    "- Lack of significant peaks does not imply a non-genetic trait\n",
    "- PRS performance is modest per individual but informative at the group level; it improves with larger training N and better methods\n",
    "- Standardization and honest holdout evaluation prevent leakage and overstatement of accuracy\n",
    "\n",
    "**Why this matters**  \n",
    "Most complex traits and common diseases are polygenic. Even modest PRS can stratify risk and inform research, screening, and trial enrichment when used responsibly.\n",
    "\n",
    "**Taking it further**\n",
    "- Tune K via cross-validation; add LD-aware/shrinkage methods (clump+threshold, ridge/BLUP, LDpred, PRS-CS, lassosum)\n",
    "- Increase discovery sample size or use external GWAS summary stats\n",
    "- Include covariates (age/sex/PCs), check ancestry transferability, and assess calibration/clinical utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0e919",
   "metadata": {},
   "source": [
    "# Complete the Reflection & Comparison Questions in Shared Slides in Groups in Canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
