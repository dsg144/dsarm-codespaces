{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d03bf6",
   "metadata": {},
   "source": [
    "# Case 4: Hybrid Architecture (Few Large + Many Small Effects)\n",
    "\n",
    "This notebook follows the GWAS→PRS workflow but simulates a trait with a mixture of genetic effects: a few SNPs with strong effects plus many with tiny effects. This scenario reflects many real-world complex traits which have both \"major genes\" and polygenic background. We expect to see both clear genome-wide significant signals and additional polygenic signal that can be captured through broader PRS approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e8b8b",
   "metadata": {},
   "source": [
    "### Step 0: Imports\n",
    "\n",
    "Import required libraries for simulation, computation and plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730903ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display\n",
    "\n",
    "# Style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46069ec9",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Genotypes & Phenotypes\n",
    "This case simulates a hybrid architecture: a few SNPs with large effects on the trait plus many SNPs with small effects. We split individuals 70/30 into discovery (training) and target (testing) sets. Noise is modest so genetics explains a large share of variance.\n",
    "\n",
    "Configuration (dataclass):\n",
    "- num_snps: total SNPs (default 1000)\n",
    "- num_individuals: total people (split 70/30 into discovery/target)\n",
    "- num_large_effect: number of large-effect SNPs (e.g., 5)\n",
    "- num_small_effect: number of small-effect SNPs (e.g., 50)\n",
    "- large_effect_size: per-allele effect for large-effect SNPs (e.g., 0.5)\n",
    "- small_effect_size: per-allele effect for small-effect SNPs (e.g., 0.1)\n",
    "- n_permutations: number of shuffles to build the empirical max |r| threshold (used in Step 4)\n",
    "- broad_PGS_k: how many top-|r| SNPs to aggregate into the broad PRS (used in Step 5)\n",
    "- noise_std: SD of environmental noise (set smaller here to yield high heritability)\n",
    "\n",
    "What are we making?\n",
    "- Genotypes: 0 / 1 / 2 copies of an allele at 1,000 SNPs\n",
    "- Phenotype: Genetic component from 5 large-effect SNPs and 50 small-effect SNPs + modest noise\n",
    "\n",
    "Why two groups?\n",
    "- Discovery (train): estimate SNP–trait associations used to create the PRS\n",
    "- Target (test): honest evaluation of the PRS\n",
    "\n",
    "How do we simulate?\n",
    "1) Draw allele frequency per SNP (uniform 0.05–0.5)\n",
    "2) Generate genotypes (0/1/2) via binomial draws\n",
    "3) Assign the first 5 SNPs large effects and the next 50 small effects; others are null\n",
    "4) Compute g = G·β and add Gaussian noise (noise_std=0.5)\n",
    "5) Use discovery to compute SNP–trait correlations; target is held out for PRS evaluation\n",
    "\n",
    "**Beginner prompts (try these for deeper understanding):**\n",
    "- “How do allele frequencies determine the probabilities of 0/1/2 genotypes?”  \n",
    "- “What happens if I change the number of causal SNPs (e.g., 50 vs 300 vs 600)?”  \n",
    "- \"How does having both large and small effects impact our ability to detect signals?\"\n",
    "- \"Why might a broad PRS outperform a strict PRS in some hybrid traits?\"\n",
    "- \"What real-world traits might follow this hybrid pattern?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce302f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Config + Polygenic Simulation + Train Preview\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "@dataclass\n",
    "class SimulationConfig:\n",
    "    num_snps: int = 1000\n",
    "    num_individuals: int = 1000  # Increase from 600 for better power\n",
    "    num_large_effect: int = 5    # New parameter\n",
    "    num_small_effect: int = 50   # New parameter\n",
    "    large_effect_size: float = 0.5  # New parameter\n",
    "    small_effect_size: float = 0.1  # New parameter\n",
    "    noise_std: float = 0.5       # Reduced noise (from 1.0)\n",
    "    n_permutations: int = 100\n",
    "    broad_PGS_k: int = 100       # Increased from 50\n",
    "\n",
    "config = SimulationConfig()\n",
    "rng = np.random.default_rng(45)  # Different seed\n",
    "\n",
    "\n",
    "# 1. Draw allele frequencies and genotypes (0/1/2)\n",
    "allele_freqs = rng.uniform(0.05, 0.5, size=config.num_snps)\n",
    "geno = np.empty((config.num_individuals, config.num_snps), dtype=np.int8)\n",
    "for j, p in enumerate(allele_freqs):\n",
    "    geno[:, j] = rng.binomial(2, p, size=config.num_individuals)\n",
    "\n",
    "# 2. Split into discovery (train) and target (test) sets\n",
    "n_train = int(0.7 * config.num_individuals)\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, config.num_individuals)\n",
    "geno_train, geno_test = geno[train_idx], geno[test_idx]\n",
    "\n",
    "# 3. Define causal SNPs: large-effect + small-effect\n",
    "large_effect_idx = np.arange(config.num_large_effect)  # First 5 SNPs\n",
    "small_effect_idx = np.arange(config.num_large_effect, \n",
    "                          config.num_large_effect + config.num_small_effect)  # Next 50 SNPs\n",
    "all_causal_idx = np.concatenate([large_effect_idx, small_effect_idx])\n",
    "\n",
    "# Setup effect sizes\n",
    "beta = np.zeros(config.num_snps)\n",
    "beta[large_effect_idx] = config.large_effect_size\n",
    "beta[small_effect_idx] = config.small_effect_size\n",
    "# 4. Genetic component + noise with reduced variance\n",
    "g_train = geno_train @ beta\n",
    "g_test = geno_test @ beta\n",
    "\n",
    "phen_train = g_train + rng.normal(0.0, config.noise_std, size=n_train)\n",
    "phen_test = g_test + rng.normal(0.0, config.noise_std, size=config.num_individuals - n_train)\n",
    "\n",
    "# Package for later steps\n",
    "data = {\n",
    "    \"allele_freqs\": allele_freqs,\n",
    "    \"geno_train\": geno_train,\n",
    "    \"geno_test\": geno_test,\n",
    "    \"phen_train\": phen_train,\n",
    "    \"phen_test\": phen_test,\n",
    "    \"large_effect_snps\": large_effect_idx,\n",
    "    \"small_effect_snps\": small_effect_idx,\n",
    "    \"all_causal_snps\": all_causal_idx,\n",
    "}\n",
    "\n",
    "# Diagnostics (empirical h2 from train split)\n",
    "var_g = g_train.var()\n",
    "var_y = phen_train.var()\n",
    "emp_h2 = var_g / var_y if var_y > 0 else np.nan\n",
    "print(f\"Train phenotype mean/var: {phen_train.mean():.3f} / {phen_train.var():.3f}\")\n",
    "print(f\"Empirical h^2 ≈ {emp_h2:.3f}\")\n",
    "print(f\"Large-effect SNPs: {len(large_effect_idx)} | Small-effect SNPs: {len(small_effect_idx)}\")\n",
    "\n",
    "# Preview first 5 rows\n",
    "preview_snps = 8\n",
    "df_train = pd.DataFrame(geno_train[:5, :preview_snps],\n",
    "                        columns=[f\"SNP_{i:04d}\" for i in range(preview_snps)])\n",
    "df_train[\"phen_train\"] = phen_train[:5]\n",
    "print(\"Train (wide) first rows:\")\n",
    "display(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff7ea6",
   "metadata": {},
   "source": [
    "### Interpreting the Training Set Preview\n",
    "\n",
    "You’re seeing only the discovery (training) table. The target (test) data exists but isn’t shown here.\n",
    "\n",
    "- Columns\n",
    "  - SNP_0000, SNP_0001, …: Genotypes coded 0/1/2.\n",
    "  - phen_train: Simulated trait = genetic component g + noise, where g = Σ_j (β_j × genotype_ij) with 5 large-effect SNPs and 50 small-effect SNPs.\n",
    "\n",
    "- effect_size (what it means)\n",
    "  - Per-allele additive change in the phenotype (slope). Moving 0→1→2 copies adds ≈ effect_size each step.\n",
    "  - Simulation vs real data: in this notebook we choose effect_size (β). In real studies, β is unknown and estimated (β̂) via GWAS (per‑SNP regression), with standard errors and p‑values.\n",
    "  - Larger |effect_size| generally produces bigger |r| and higher PRS R², but detectability also depends on sample size, allele frequency, and noise. In this notebook we use r as a simple proxy weight for β̂.\n",
    "  \n",
    "- Key points\n",
    "  - By construction, SNPs 0–4 have large effects; SNPs 5–54 have small effects; others are null.\n",
    "  - Train and test are separate draws; don’t use test data until evaluation.\n",
    "  - We expect a couple of strong SNP signals plus broader sub-threshold signal from the many small effects.\n",
    "\n",
    "- Next\n",
    "  - Standardize (z-score) to compare SNP–trait correlations on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ad372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the ground truth effect sizes\n",
    "plt.figure(figsize=(8, 3))\n",
    "x = np.arange(config.num_large_effect + config.num_small_effect)\n",
    "y = np.concatenate([\n",
    "    np.ones(config.num_large_effect) * config.large_effect_size,\n",
    "    np.ones(config.num_small_effect) * config.small_effect_size\n",
    "])\n",
    "markerline, stemlines, baseline = plt.stem(x, y)  # removed use_line_collection\n",
    "plt.setp(markerline, markersize=4, color='tab:blue')\n",
    "plt.setp(stemlines, linewidth=1, color='tab:blue')\n",
    "plt.axhline(0, color='gray', linestyle='-', alpha=0.3)\n",
    "plt.ylabel('Effect Size (per allele)')\n",
    "plt.xlabel('Causal SNP Index (0..{} large, then small)'.format(config.num_small_effect))\n",
    "plt.title('True Effect Sizes (Hybrid Architecture)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c967c2c",
   "metadata": {},
   "source": [
    "### Step 2: Standardization\n",
    "\n",
    "We now convert raw genotype and phenotype values into **z-scores** in the discovery set.  A z-score answers the question: *\"How many standard deviations above (+) or below (-) the mean is this value?\"*\n",
    "\n",
    "**Formula:**  \n",
    "$z = (value - mean) / \\mathrm{SD}$\n",
    "\n",
    "**Why we do this before computing correlations:**\n",
    "- Puts every SNP column and the phenotype on the **same scale** (mean 0, SD 1).\n",
    "- Makes Pearson’s $r$ simply the **average product** of two standardized variables, which acts like an effect size.\n",
    "- Allows us to compare SNP signals fairly — rare and common variants are no longer unfairly scaled.\n",
    "- Keeps the *ordering* of individuals unchanged; we merely shift and rescale.\n",
    "\n",
    "In a polygenic context with thousands of small effects, standardization is even more important: it ensures that none of the weak signals is artificially inflated or deflated simply due to allele frequency or measurement scale differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Standardize Variables (Genotypes + Phenotypes)\n",
    "\n",
    "def standardize_matrix(mat: np.ndarray):  # -> Tuple[np.ndarray, np.ndarray, np.ndarray]\n",
    "    mean = mat.mean(axis=0)\n",
    "    std = mat.std(axis=0)\n",
    "    std[std == 0] = 1.0\n",
    "    return (mat - mean) / std, mean, std\n",
    "\n",
    "Z_geno_train, geno_mean_train, geno_std_train = standardize_matrix(data['geno_train'])\n",
    "Z_geno_test, geno_mean_test_raw, geno_std_test_raw = standardize_matrix(data['geno_test'])  # independent standardization\n",
    "\n",
    "phen_train = data['phen_train']\n",
    "phen_test = data['phen_test']\n",
    "Z_phen_train = (phen_train - phen_train.mean()) / phen_train.std()\n",
    "Z_phen_test = (phen_test - phen_test.mean()) / phen_test.std()\n",
    "\n",
    "# Diagnostics to show why raw vs standardized look similar\n",
    "print(\"Raw phenotype   mean/std = {:.3f} / {:.3f}\".format(phen_train.mean(), phen_train.std()))\n",
    "print(\"Standardized    mean/std = {:.3f} / {:.3f}\".format(Z_phen_train.mean(), Z_phen_train.std()))\n",
    "print(\"Raw   min/max = {:.3f} / {:.3f}\".format(phen_train.min(), phen_train.max()))\n",
    "print(\"Z     min/max = {:.3f} / {:.3f}\".format(Z_phen_train.min(), Z_phen_train.max()))\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "# Raw distribution\n",
    "axes[0].hist(phen_train, bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(phen_train.mean(), color='k', linestyle='--', linewidth=1, label='Mean')\n",
    "axes[0].set_title('Raw Phenotype (Discovery)')\n",
    "axes[0].set_xlabel('Phenotype')\n",
    "axes[0].legend()\n",
    "\n",
    "# Standardized distribution (fixed axis to emphasize z-scale)\n",
    "axes[1].hist(Z_phen_train, bins=30, color='salmon', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].set_xlim(-4,4)\n",
    "axes[1].set_title('Standardized Phenotype (Discovery)')\n",
    "axes[1].set_xlabel('Z-Phenotype')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd10f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional viz: one SNP before/after standardization (discovery set)\n",
    "# Pick a SNP with allele frequency near 0.30 for variability\n",
    "p = data['allele_freqs']\n",
    "idx_snp = int(np.argmin(np.abs(p - 0.30)))\n",
    "x_raw = data['geno_train'][:, idx_snp]\n",
    "x_z = Z_geno_train[:, idx_snp]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "# Left: raw genotype distribution (0/1/2)\n",
    "axes[0].hist(x_raw, bins=[-0.5, 0.5, 1.5, 2.5], color='steelblue', edgecolor='black')\n",
    "axes[0].set_xticks([0,1,2])\n",
    "axes[0].axvline(x_raw.mean(), color='k', linestyle='--', linewidth=1, label=f\"Mean={x_raw.mean():.2f}\")\n",
    "axes[0].set_title(f\"SNP_{idx_snp:04d} (raw)\")\n",
    "axes[0].set_xlabel('Genotype (0/1/2)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(frameon=False)\n",
    "\n",
    "# Right: standardized genotype distribution (z-scores)\n",
    "axes[1].hist(x_z, bins=30, color='indianred', edgecolor='black', density=True)\n",
    "axes[1].axvline(0, color='k', linestyle='--', linewidth=1, label='Mean=0')\n",
    "axes[1].axvline(1, color='gray', linestyle=':', linewidth=1, label='±1 SD')\n",
    "axes[1].axvline(-1, color='gray', linestyle=':', linewidth=1)\n",
    "axes[1].set_title(f\"SNP_{idx_snp:04d} (standardized)\")\n",
    "axes[1].set_xlabel('Z-score')\n",
    "axes[1].legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefc799",
   "metadata": {},
   "source": [
    "### Step 3: GWAS Correlations\n",
    "\n",
    "**Goal.** Quantify the relationship between each SNP and the trait.\n",
    "\n",
    "What to expect in Case 4 (hybrid):\n",
    "- A few SNPs (the large-effect ones) with noticeably high |r| that should clear threshold.\n",
    "- Many small-effect SNPs with modest |r| (often sub-threshold) that still contribute in aggregate via PRS.\n",
    "\n",
    "**What we do.** For each SNP $j$, we compute the Pearson correlation $r_j$ between its standardized genotypes and the standardized phenotype in the discovery set.  Because both are z-scored, $r_j$ is just the average product of two series of standardized numbers.\n",
    "\n",
    "**How to read $r$:**\n",
    "- $r_j \\approx 0$ means no detectable association.\n",
    "- $r_j > 0$ means individuals with more minor alleles tend to have **higher** trait values.\n",
    "- $r_j < 0$ means individuals with more minor alleles tend to have **lower** trait values.\n",
    "- Larger $|r_j|$ implies a stronger SNP–trait link on a common scale (and $r_j^2$ is the in-sample variance explained by SNP $j$).\n",
    "\n",
    "**Why so small?**  In a diffuse polygenic architecture, each true causal SNP has a tiny effect.  Their $|r_j|$ values are buried in sampling noise, so the largest observed $|r_j|$ will typically be modest (≈ 0.17) and may not exceed the permutation threshold.  The important information is in the **ranking** of $|r_j|$, not the individual magnitude.\n",
    "\n",
    "**Intuition.**  To decide if a SNP and trait move together, imagine multiplying their standardized values person-by-person and averaging: if the values tend to be above average together, the average product is positive; if one tends to be high when the other is low, the average product is negative; if there’s no pattern, the average product is near zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: GWAS via per-SNP correlations\n",
    "\n",
    "r_values = (Z_geno_train * Z_phen_train[:, None]).mean(axis=0)\n",
    "r_abs = np.abs(r_values)\n",
    "max_r = r_abs.max()\n",
    "print(f\"Maximum |r| = {max_r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Step 3 viz: one SNP vs phenotype (discovery set)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = int(np.argmax(r_abs))  # SNP with largest |r|\n",
    "snp_name = f\"SNP_{idx:04d}\"\n",
    "is_large = idx in data['large_effect_snps']\n",
    "is_small = idx in data['small_effect_snps']\n",
    "effect_type = \"large-effect\" if is_large else \"small-effect\" if is_small else \"null\"\n",
    "\n",
    "x_raw = data['geno_train'][:, idx]\n",
    "y_raw = phen_train\n",
    "x_z = Z_geno_train[:, idx]\n",
    "y_z = Z_phen_train\n",
    "r = float(r_values[idx]); r2 = r*r\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "jit = (rng.random(x_raw.size) - 0.5) * 0.12  # jitter to separate 0/1/2 columns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "# Raw scale: shows per-allele shifts\n",
    "axes[0].scatter(x_raw + jit, y_raw, s=8, alpha=0.5)\n",
    "means = [y_raw[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[0].plot([0,1,2], means, 'r-o', lw=2, label='Group means')\n",
    "axes[0].set_xticks([0,1,2]); axes[0].legend(frameon=False)\n",
    "axes[0].set_xlabel('Genotype (0/1/2)'); axes[0].set_ylabel('Phenotype (raw)')\n",
    "axes[0].set_title(f'{snp_name} vs phen_train ({effect_type})')\n",
    "\n",
    "# Z scale: matches how r is computed\n",
    "axes[1].scatter(x_z + jit, y_z, s=8, alpha=0.5)\n",
    "# Group means in z space (plot at mean x_z for each genotype 0/1/2)\n",
    "x_means = [x_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "y_means = [y_z[x_raw==g].mean() if np.any(x_raw==g) else np.nan for g in [0,1,2]]\n",
    "axes[1].plot(x_means, y_means, 'r-o', lw=2, label='Group means')\n",
    "axes[1].legend(frameon=False)\n",
    "axes[1].set_xlabel('Genotype (z-scored)'); axes[1].set_ylabel('Phenotype (z-scored)')\n",
    "axes[1].set_title(f'{snp_name} (|r|={abs(r):.2f}, R²={r2:.2f})')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adc5ba1",
   "metadata": {},
   "source": [
    "### How the scatterplot connects to Step 3 correlations - Hybrid Case\n",
    "\n",
    "- Left panel (raw):\n",
    "  - Three vertical bands (genotype 0/1/2). The red \"group means\" line shows the average phenotype for each genotype.\n",
    "  - This hybrid architecture contains both large-effect and small-effect SNPs.\n",
    "  - The SNP shown is likely a large-effect variant, showing a clear additive trend across genotype groups.\n",
    "  - The slope (per-allele effect) is steep for large-effect SNPs but would be more modest for small-effect SNPs.\n",
    "\n",
    "- Right panel (z-scored):\n",
    "  - Both axes are standardized, which is exactly how we compute r in Step 3.\n",
    "  - Pearson r for this SNP is the average product r = mean(x_z · y_z).\n",
    "  - The higher |r| value (typically ~0.2-0.3 for large-effect SNPs) indicates a stronger genotype-phenotype association.\n",
    "  - Large-effect SNPs explain more variance individually (higher R²), while small-effect SNPs contribute collectively.\n",
    "\n",
    "- Why this matters:\n",
    "  - In the Manhattan plot, we'll see clear peaks for large-effect SNPs rising above the threshold.\n",
    "  - Small-effect SNPs create a slightly elevated \"background\" level compared to pure noise.\n",
    "  - This hybrid architecture allows us to compare both significance-based (strict) and ranking-based (broad) PRS approaches.\n",
    "  - The r values for large-effect SNPs provide strong weights for the PRS, while smaller effects contribute additively.\n",
    "\n",
    "Note: The hybrid architecture combines features of both Case 1 (sparse) and Case 2 (polygenic), with a mix of clear strong signals and numerous weaker signals that together improve prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c89acc",
   "metadata": {},
   "source": [
    "### Mini-GWAS framing (what we’re mimicking)\n",
    "\n",
    "- A GWAS tests each SNP across the genome for association with a phenotype (one SNP at a time).  \n",
    "- Real GWAS uses regression (**β, SE, p-value**) and includes covariates (age, sex, ancestry PCs) to control confounding.  \n",
    "- In this exercise, we use **z-scores** and compute a simple **per-SNP correlation (r)** as a stand-in for GWAS effect size.  \n",
    "- Output of this step is a “summary stats–like” table: **SNP ID, r, |r|**.  \n",
    "- Next, we’ll visualize all SNPs together (Manhattan plot) and set a significance cut line via **permutation**—analogous to GWAS genome-wide thresholds.  \n",
    "\n",
    "**TL;DR:** Step 3 ≈ a **mini-GWAS** pass over SNPs to get per-SNP effects we can carry forward.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619476da",
   "metadata": {},
   "source": [
    "### Step 4: Permutation (simple) + Manhattan Plot (Hybrid)\n",
    "\n",
    "From one SNP to all SNPs\n",
    "- Step 3 computed an r per SNP; the Manhattan plot shows |r| across all SNPs on a common z-scale.\n",
    "\n",
    "What we do (simple permutation)\n",
    "- We ask: “How tall could the biggest |r| be just by chance?”\n",
    "- Shuffle the phenotype B times.\n",
    "- Each time, compute |r| across SNPs and record the maximum.\n",
    "- Use the 95th percentile of these maxima as the dashed threshold.\n",
    "\n",
    "How to read the Manhattan (hybrid)\n",
    "- Dots = SNPs; height = |r|. Dashed orange line = permutation threshold.\n",
    "- Purple stars = Top‑K by |r| (broad PRS).\n",
    "- Expect a few peaks above the line (large effects) plus a slightly elevated “floor” from many small effects.\n",
    "\n",
    "PRS note\n",
    "- Strict PRS: SNPs ≥ threshold. Broad PRS: Top‑K by |r|.\n",
    "\n",
    "Beginner tip\n",
    "- “Mountains rising from hills”: big peaks (large effects) on top of a low, rolling background (small effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compact permutation + Manhattan (single line + Top‑K overlay)\n",
    "def perm_threshold(Z_geno: np.ndarray, y: np.ndarray, B: int = 100, q: float = 0.95, seed: int = 0) -> float:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    max_abs = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        y_shuf = rng.permutation(y)\n",
    "        r_shuf = (Z_geno * y_shuf[:, None]).mean(axis=0)\n",
    "        max_abs[b] = np.abs(r_shuf).max()\n",
    "    return float(np.quantile(max_abs, q))\n",
    "\n",
    "ALPHA = 0.05\n",
    "B = config.n_permutations if hasattr(config, \"n_permutations\") else 100\n",
    "threshold = perm_threshold(Z_geno_train, Z_phen_train, B=B, q=1-ALPHA, seed=0)\n",
    "\n",
    "# Selection summary for strict vs broad PRS\n",
    "r_abs = np.abs(r_values)\n",
    "topK = config.broad_PGS_k\n",
    "topK_idx = np.argsort(r_abs)[-topK:]\n",
    "selected_idx = np.flatnonzero(r_abs >= threshold)\n",
    "\n",
    "print(f\"Permutation threshold (95%): {threshold:.3f} | Above threshold: {selected_idx.size} | Top-K (broad PRS) = {topK}\")\n",
    "\n",
    "# Manhattan plot (hybrid)\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(config.num_snps)\n",
    "plt.scatter(x, r_abs, s=10, c='black', alpha=0.6, label='All SNPs')\n",
    "plt.scatter(topK_idx, r_abs[topK_idx], s=40, c='purple', alpha=0.9, marker='*', label=f'Top {topK} by |r| (broad PRS)')\n",
    "plt.axhline(threshold, color='orange', linestyle='--', linewidth=1.5, label='95% permutation threshold')\n",
    "plt.xlabel('SNP index'); plt.ylabel('Absolute correlation |r|')\n",
    "plt.title('Manhattan Plot (Hybrid: threshold + Top‑K overlay)')\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068004f",
   "metadata": {},
   "source": [
    "### From GWAS results to Manhattan (our version)\n",
    "\n",
    "- Classic GWAS Manhattan plots **−log10(p)** by genomic position; taller peaks = stronger evidence.  \n",
    "- Here, we plot **|r|** instead of −log10(p). The goal is the same: a genome-wide view of signal strength.  \n",
    "- GWAS uses fixed significance lines (e.g., *5×10⁻⁸*). We use a **permutation-based line** that reflects our data.  \n",
    "- Selection rule: take all SNPs above the line; if none cross, use a **top-K by |r| fallback** (keep the sign of r for direction).  \n",
    "- Same idea, simpler ingredients: our permutation line plays the role of a **GWAS significance threshold**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace0bc6",
   "metadata": {},
   "source": [
    "### Step 5: Build and Evaluate a Polygenic Score\n",
    "\n",
    "Goal: Compare a strict PRS using only genome-wide hits vs a broad PRS that also captures small effects.\n",
    "\n",
    "What we do:\n",
    "1. Strict PRS: include SNPs that pass the permutation threshold (should be ~5 large-effect SNPs)\n",
    "2. Broad PRS: include top-K SNPs by |r| (e.g., K=100) to capture large + small effects\n",
    "3. Evaluate performance: correlation (r, R²) and decile stratification\n",
    "4. Visualize: Scatter and decile comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b58ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Build and evaluate polygenic scores (strict and broad)\n",
    "\n",
    "# Strict PRS: only SNPs passing threshold\n",
    "sig_idx = np.where(r_abs >= threshold)[0]\n",
    "strict_label = f'Strict ({len(sig_idx)} SNPs)'\n",
    "strict_prs = Z_geno_test[:, sig_idx] @ r_values[sig_idx] if len(sig_idx) > 0 else np.zeros(len(Z_phen_test))\n",
    "strict_prs = (strict_prs - strict_prs.mean()) / strict_prs.std() if strict_prs.std() > 0 else strict_prs\n",
    "\n",
    "# Broad PRS: top-K SNPs regardless of threshold  \n",
    "broad_idx = np.argsort(r_abs)[-config.broad_PGS_k:]\n",
    "broad_label = f'Broad (top-{config.broad_PGS_k})'\n",
    "broad_prs = Z_geno_test[:, broad_idx] @ r_values[broad_idx]\n",
    "broad_prs = (broad_prs - broad_prs.mean()) / broad_prs.std()\n",
    "\n",
    "# Evaluate both against phenotype\n",
    "strict_R = float(np.corrcoef(strict_prs, Z_phen_test)[0,1]) if strict_prs.std() > 0 else 0.0\n",
    "broad_R = float(np.corrcoef(broad_prs, Z_phen_test)[0,1])\n",
    "strict_R2 = strict_R * strict_R\n",
    "broad_R2 = broad_R * broad_R\n",
    "\n",
    "print(f\"Significant SNPs (threshold): {len(sig_idx)}\")\n",
    "print(\"PRS comparison:\")\n",
    "print(f\"  - {strict_label}: r = {strict_R:.3f}, R² = {strict_R2:.3f}\")\n",
    "print(f\"  - {broad_label}: r = {broad_R:.3f}, R² = {broad_R2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare scatter plots\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(strict_prs, Z_phen_test, s=8, alpha=0.4, color='orangered')\n",
    "plt.xlabel('Strict PRS (z-score)')\n",
    "plt.ylabel('Phenotype (z-score)')\n",
    "plt.title(f'Strict PRS (r={strict_R:.2f}, R²={strict_R2:.2f})')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(broad_prs, Z_phen_test, s=8, alpha=0.4, color='royalblue')\n",
    "plt.xlabel('Broad PRS (z-score)')\n",
    "plt.ylabel('Phenotype (z-score)')\n",
    "plt.title(f'Broad PRS (r={broad_R:.2f}, R²={broad_R2:.2f})')\n",
    "\n",
    "# Create side-by-side decile plots\n",
    "plt.subplot(1, 3, 3)\n",
    "\n",
    "# Create strict PRS deciles\n",
    "edges_strict = np.quantile(strict_prs, np.linspace(0,1,11))\n",
    "dec_strict = np.digitize(strict_prs, edges_strict[1:-1], right=True)\n",
    "mean_strict = [Z_phen_test[dec_strict == d].mean() if np.sum(dec_strict==d) > 0 else np.nan for d in range(10)]\n",
    "se_strict = [Z_phen_test[dec_strict == d].std(ddof=1)/np.sqrt(np.sum(dec_strict==d)) \n",
    "            if np.sum(dec_strict==d) > 1 else np.nan for d in range(10)]\n",
    "gap_strict = mean_strict[-1] - mean_strict[0]\n",
    "\n",
    "# Create broad PRS deciles\n",
    "edges_broad = np.quantile(broad_prs, np.linspace(0,1,11))\n",
    "dec_broad = np.digitize(broad_prs, edges_broad[1:-1], right=True)\n",
    "mean_broad = [Z_phen_test[dec_broad == d].mean() if np.sum(dec_broad==d) > 0 else np.nan for d in range(10)]\n",
    "se_broad = [Z_phen_test[dec_broad == d].std(ddof=1)/np.sqrt(np.sum(dec_broad==d)) \n",
    "           if np.sum(dec_broad==d) > 1 else np.nan for d in range(10)]\n",
    "gap_broad = mean_broad[-1] - mean_broad[0]\n",
    "\n",
    "# Plot both decile trends\n",
    "plt.errorbar(range(1,11), mean_strict, yerr=se_strict, fmt='-o', capsize=3, \n",
    "            color='orangered', label=f'Strict PRS (Δ₁₀₋₁={gap_strict:.2f})')\n",
    "plt.errorbar(range(1,11), mean_broad, yerr=se_broad, fmt='-o', capsize=3, \n",
    "            color='royalblue', label=f'Broad PRS (Δ₁₀₋₁={gap_broad:.2f})')\n",
    "plt.title(f'PRS Decile Comparison')\n",
    "plt.xlabel('PRS Decile (1=lowest, 10=highest)')\n",
    "plt.ylabel('Mean Phenotype (z-score)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5654d",
   "metadata": {},
   "source": [
    "### Step 5 Interpretation\n",
    "\n",
    "**Performance metrics:**\n",
    "- Strict PRS: Using only significant SNPs (approximately 5)\n",
    "  - Correlation: r ≈ 0.7, R² ≈ 0.49\n",
    "  \n",
    "- Broad PRS: Using top 100 SNPs by |r|\n",
    "  - Correlation: r ≈ 0.75, R² ≈ 0.56\n",
    "\n",
    "**What this means:**\n",
    "- **Clear strong signals**: The strict PRS captures the large-effect SNPs that stand out in the Manhattan plot\n",
    "- **Additional polygenic signal**: The broad PRS improves performance by including small-effect variants\n",
    "- **Two-tier genetic architecture**: This hybrid scenario demonstrates that both approaches have value\n",
    "\n",
    "**Understanding the decile comparison:**\n",
    "- Both PRS models show strong stratification across deciles\n",
    "- The strict PRS has a steep gradient driven by the few large effects\n",
    "- The broad PRS shows a slightly improved gradient by capturing both large and small effects\n",
    "- The gap between lowest and highest deciles (Δ₁₀-₁) is substantially larger than in purely polygenic traits\n",
    "\n",
    "**Why this matters:**\n",
    "This hybrid architecture represents many real-world complex traits where a few loci have outsized influence while numerous small effects contribute to the polygenic background. Understanding both components can inform targeted interventions (for major genes) and population-level approaches (for polygenic background)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbb4ed",
   "metadata": {},
   "source": [
    "### Conclusion: What We Learned from Case 4\n",
    "\n",
    "**Journey summary**  \n",
    "We explored a hybrid genetic architecture combining few large-effect variants with many small-effect variants. This architecture mirrors many real-world complex traits that have both \"major genes\" and polygenic background.\n",
    "\n",
    "**What we accomplished**\n",
    "1. Simulated a trait with dual genetic architecture: 5 strong-effect SNPs plus 50 small-effect SNPs\n",
    "2. Standardized variables to enable fair comparisons across SNPs of varying frequencies\n",
    "3. Observed a Manhattan plot with clear peaks (large effects) and a \"polygenic floor\" (small effects)\n",
    "4. Evaluated two PRS approaches: strict (significant SNPs only) and broad (top 100 SNPs)\n",
    "5. Demonstrated that while the strict PRS performs well, the broad approach captures additional variance\n",
    "6. Visualized how both approaches effectively stratify individuals across risk deciles\n",
    "\n",
    "**Key insights**\n",
    "- Hybrid architectures benefit from two-tier analytic approaches\n",
    "- The strict PRS offers interpretability by focusing on validated signals\n",
    "- The broad PRS maximizes predictive power by capturing both large and small effects\n",
    "- The optimal approach depends on the specific goals (explanation vs prediction)\n",
    "- Decile stratification is particularly strong in hybrid traits, offering clear risk differentiation\n",
    "\n",
    "**Why this matters**  \n",
    "Many clinically relevant traits show this hybrid architecture - examples include conditions like type 2 diabetes, breast cancer, and heart disease, where known major risk variants exist alongside polygenic background. Understanding both components can inform both targeted interventions and population-level risk assessment.\n",
    "\n",
    "**Taking it further**\n",
    "- Explore differential weighting schemes that prioritize validated variants\n",
    "- Investigate prediction performance across different population backgrounds\n",
    "- Consider gene-gene interaction effects between major variants and polygenic background\n",
    "- Evaluate prediction accuracy across different segments of the risk distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2478e9c",
   "metadata": {},
   "source": [
    "**Comparison to Other Cases**\n",
    "- Unlike Case 1 (sparse), signal comes from both major and minor genetic factors\n",
    "- Unlike Case 2 (polygenic), some SNPs are clearly detectable on their own\n",
    "- This hybrid architecture combines the benefits of both: clear targets for follow-up (large effects) and improved prediction through aggregation (small effects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0e919",
   "metadata": {},
   "source": [
    "# Complete the Reflection & Comparison Questions in Shared Slides in Groups in Canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
